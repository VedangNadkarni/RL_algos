{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "permalink: /a2c/\n",
    "layout: notebook\n",
    "author_profile: true\n",
    "title: Actor-Critic Methods, Advantage Actor-Critic (A2C) and Generalized Advantage Estimation (GAE)\n",
    "folder: \"a2c\"\n",
    "ipynb: \"a2c.ipynb\"\n",
    "excerpt: ##########################\n",
    "header:\n",
    "  teaser: /assets/a2c/######################\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REINFORCE Review\n",
    "\n",
    "In the [previous post](% post_url 2020-07-10-reinforce %) we introduce REINFORCE, an **on-policy policy gradient algorithm**. The policy gradient theorem gives the gradient of the sum of returns for a rollout with respect to the policy parameters $\\theta$:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E}_\\tau \\left[ \\sum_{t=0}^T \\nabla_\\theta \\log \\pi_\\theta (a_t \\mid s_t) Q^{\\pi_\\theta}(s_t, a_t) \\right]\n",
    "$$\n",
    "\n",
    "REINFORCE samples this gradient estimate by performing a series of $N$ **rollouts** in the environment to collect trajectories $\\tau$, and then computing the **empirical returns** $G_t$ as a substitute for $Q^{\\pi_\\theta}(s_t, a_t)$. \n",
    "For each rollout we then compute the gradient $\\nabla_\\theta \\log \\pi_\\theta(a_t \\mid s_t)$ for each time step, and sum over timesteps to get an estimate for the policy gradient. We then average over rollouts to get a less noisy estimate for the gradient than a single rollout alone.\n",
    "\n",
    "$$\n",
    "\\hat{g} = \\frac{1}{N} \\sum_{n=1}^N \\sum_{t=0}^T \\nabla_\\theta \\log \\pi_\\theta (a_t \\mid s_t) G_t\n",
    "$$\n",
    "\n",
    "We update the policy parameters by stochastic gradient ascent (or some other gradient-based optimizer):\n",
    "\n",
    "$$\n",
    "\\theta \\gets \\theta + \\alpha \\hat{g}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baselines\n",
    "\n",
    "One issue with using the empirical returns $G_t$ directly is that the scale of the returns can be quite large depending on the scale of the rewards, the value of $\\gamma$, and the time horizon $T$ of the environment. For example, in the `CartPole-v0` problem, the time horizon is $T=200$ and the rewards are $r_t = 1$. The maximum return is then \n",
    "\n",
    "$$\n",
    "G_0 = \\sum_{t=0}^{199} 1 \\gamma^t = \\frac{1-\\gamma^200}{1-\\gamma}\n",
    "$$\n",
    "\n",
    "which for $\\gamma = 0.99$ is about $181$. Returns with large scale like this can be highly variable, and as a consequence our estimates for the gradient can be highly variable. \n",
    "\n",
    "It turns out that we can modify the policy gradient theorem to reduce the variance!\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E}_\\tau \\left[ \\sum_{t=0}^T \\log \\pi_\\theta(s_t \\mid a_t) (Q^{\\pi_\\theta}(s_t, a_t) - b(s_t)) \\right]\n",
    "$$\n",
    "\n",
    "we can subtract a **baseline** $b(s_t)$, which can be any function that depends on the state $s_t$.\n",
    "\n",
    "Let's prove that the expectation of the gradient does not change:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\nabla_\\theta J(\\theta) &= \\mathbb{E}_\\tau \\left[  \\sum_{t=0}^T \\nabla_\\theta \\log \\pi_\\theta (s_t, a_t)(Q^{\\pi_\\theta}(s_t, a_t) - b(s_t) \\right] \\\\\n",
    "&= \\mathbb{E}_\\tau \\left[  \\sum_{t=0}^T \\nabla_\\theta \\log \\pi_\\theta (s_t, a_t)Q^{\\pi_\\theta}(s_t, a_t) \\right]  -  \\mathbb{E}_\\tau \\left[  \\sum_{t=0}^T \\nabla_\\theta \\log \\pi_\\theta (s_t, a_t) b(s_t) \\right]\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first term is the original policy gradient. Let's show that the second term is equal to 0. Let's define $p(s_t \\mid \\pi_\\theta)$ to be the **visitation frequency** of $s_t$ under $pi_\\theta$ (how often we expect to see $s_t$ under our policy):\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbb{E}_\\tau \\left[  \\sum_{t=0}^T \\nabla_\\theta \\log \\pi_\\theta (s_t, a_t) b(s_t) \\right] &= \\sum_{t=0}^T \\mathbb{E}_\\tau \\left[\\nabla_\\theta \\log \\pi_\\theta (s_t, a_t) b(s_t) \\right] \\\\\n",
    "&= \\sum_{t=0}^T \\sum_{s_t \\in \\mathcal{S}} p(s_t \\mid \\pi_\\theta) \\sum_{a_t \\in \\mathcal{A}} \\nabla_\\theta \\log \\pi_\\theta (s_t, a_t) b(s_t) \\\\\n",
    "&= \\sum_{t=0}^T \\sum_{s_t \\in \\mathcal{S}} p(s_t \\mid \\pi_\\theta) b(s_t) \\sum_{a_t \\in \\mathcal{A}} \\nabla_\\theta \\log \\pi_\\theta (s_t, a_t)  \\\\\n",
    "&= \\sum_{t=0}^T \\sum_{s_t \\in \\mathcal{S}} p(s_t \\mid \\pi_\\theta) b(s_t) \\nabla_\\theta \\sum_{a_t \\in \\mathcal{A}} \\log \\pi_\\theta (s_t, a_t)  \\\\\n",
    "&= \\sum_{t=0}^T \\sum_{s_t \\in \\mathcal{S}} p(s_t \\mid \\pi_\\theta) b(s_t) \\nabla_\\theta 1 \\\\\n",
    "&= \\sum_{t=0}^T \\sum_{s_t \\in \\mathcal{S}} p(s_t \\mid \\pi_\\theta) b(s_t) 0 \\\\\n",
    "&= 0\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is **only true** because we can pull $b(s_t)$ through the summation over actions, so that they sum to $1$. This is why the baseline can be at most a function of the state $s_t$ (but never a function of the action $a_t$ as well, or this trick wouldn't be possible)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantage function\n",
    "\n",
    "But what function should we use as a baseline $b_t$? Consider the difference between $Q^{\\pi_\\theta}(s_t, a_t)$ and $V^{\\pi_\\theta}(s_t)$. $Q^{\\pi_\\theta}(s_t, a_t)$ tells us the expected return of taking an action $a_t$ in a state $s_t$, then following the policy $\\pi_\\theta$ for the rest of the trajectory. On the other hand, $V^{\\pi_\\theta}$ just tells us the expected return of beginning in a state $s_t$ and following the policy $\\pi_\\theta$ from this point onwards. If there is some **advantage** in choosing action $a_t$ over just following our policy, this can be captured in the difference between $Q^{\\pi_\\theta}(s_t, a_t)$ and $V^{\\pi_\\theta}(s_t)$:\n",
    "\n",
    "$$\n",
    "A^{\\pi_\\theta}(s_t, a_t) = Q^{\\pi_\\theta}(s_t, a_t) - V^{\\pi_\\theta}(s_t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $A^{\\pi_\\theta}(s_t, a_t)$ is called the **advantage function**. If choosing $a_t$ is better than following our policy, then $A^{\\pi_\\theta}(s_t, a_t) > 0$. If it's worse than following our policy, then $A^{\\pi_\\theta}(s_t, a_t) < 0$. If $a_t$ is exactly the action suggested by our policy, then $A^{\\pi_\\theta}(s_t, a_t) = 0$.\n",
    "\n",
    "If we use $V^{\\pi_\\theta}$ as our baseline function $b$, then the policy gradient becomes:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E}_\\tau \\left[ \\sum_{t=0}^T \\log \\pi_\\theta(s_t \\mid a_t) A^{\\pi_\\theta}(s_t, a_t) \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the advantage is actually incredibly intuitive. If the advantage is positive, then we increase the log probability of the action $a_t$ associated with that advantage. Likewise, if it's negative, then we decrease the log probability of the action $a_t$ associated with that advantage.\n",
    "\n",
    "Consider `CartPole-v0` again. The rewards are always positive, so the returns are always positive. This means that the policy updates used in REINFORCE will *always* increase the log-probability of an action. It is only capable of learning because actions with high returns have their log probability increased more than actions with low returns. We never actually try to *decrease* the log-probability of any actions. By using the advantage instead, we actually decrease the log-probability of actions that perform worse than expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantage estimation\n",
    "\n",
    "Like $Q^{\\pi_\\theta}$ and $V^{\\pi_\\theta}$, it is extremely difficult to know the advantage exactly. Instead, we usually have to make an estimate for the advantage. There are several ways that we can do this.\n",
    "\n",
    "### Estimating $Q^{\\pi_\\theta}$\n",
    "\n",
    "In deep $Q$-learning, we use a neural network $Q_\\phi$ to predict $Q^{\\pi_\\theta}$ for each action given the state $s_t$. We fit our network using the Bellman equation to generate targets:\n",
    "\n",
    "$$\n",
    "y_t = r_t + \\gamma \\max_{a_{t+1}} Q_\\phi (s_{t+1}, a_{t+1})\n",
    "$$\n",
    "\n",
    "we then regress our output by minimizing the loss\n",
    "\n",
    "$$\n",
    "L(\\phi) = \\sum_{t=0}^T (y_t -  Q_\\phi (s_t, a_t))^2\n",
    "$$\n",
    "\n",
    "Note that instead of using the Bellman equation, we could have used the empirical returns $G_t$ as the target:\n",
    "\n",
    "$$\n",
    "L(\\phi) = \\sum_{t=0}^T (G_t -  Q_\\phi (s_t, a_t))^2\n",
    "$$\n",
    "\n",
    "The advantage of using the Bellman equation is that we can use it for continuing environments, whereas empirical returns $G_t$ are only available in episodic environments.\n",
    "\n",
    "Deep $Q$-learning works on discrete action spaces by having one output of the neural network for each action. For continuous action spaces, this is not possible. One workaround is to simply provide both $s_t$ and $a_t$ into the network and produce a scalar output $Q_\\phi(s_t, a_t)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating $V^{\\pi_\\theta}$\n",
    "\n",
    "Just like in deep $Q$-learning, we can simply have a neural network $V_\\phi$ that predicts $V^{\\pi_\\theta}$ given the state. We have a Bellman equation for the value function as well, which we can use to generate targets:\n",
    "\n",
    "$$\n",
    "y_t = r_t + \\gamma V_\\phi(s_{t+1})\n",
    "$$\n",
    "\n",
    "we then regress our output by minimizing the loss\n",
    "\n",
    "$$\n",
    "L(\\phi) = \\sum_{t=0}^T (y_t -  V_\\phi (s_t))^2\n",
    "$$\n",
    "\n",
    "again, we could have just used the empirical returns if the environment is episodic.\n",
    "\n",
    "$$\n",
    "L(\\phi) = \\sum_{t=0}^T (G_t -  V_\\phi (s_t))^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating $A^{\\pi_\\theta}$ using $Q^{\\pi_\\theta}$ and $V^{\\pi_\\theta}$\n",
    "\n",
    "We can estimate the advantage $A^{\\pi_\\theta}$ by just fitting two neural networks $Q_\\phi$ and $V_{\\phi'}$ with parameters $\\phi$ and $\\phi'$. At each time step, we can take a gradient step to minimize $L(\\phi)$ and $L(\\phi')$ so that our estimate for the advantage improves over time. However, this method requires fitting two networks in addition to the policy network $\\pi_\\theta$.\n",
    "\n",
    "When using episodic environments, it can be simpler to only fit the value network $V_\\phi$, and then estimate the advantage as \n",
    "\n",
    "$$\n",
    "\\hat{A}(s_t) = G_t - V_{\\phi'}(s_t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating $A^{\\pi_\\theta}$ using $V^{\\pi_\\theta}$ alone\n",
    "\n",
    "For continuing environments, we can also get away with only fitting $V_{\\phi'}$. Define the temporal-difference error or **TD error** $\\delta_t^{\\pi_\\theta}$ to be:\n",
    "\n",
    "$$\n",
    "\\delta_t^{\\pi_\\theta} = r_t + \\gamma V^{\\pi_\\theta}(s_{t+1}) - V^{\\pi_\\theta}(s_t)\n",
    "$$\n",
    "\n",
    "This is the quantity we are minimizing when we fit $V_{\\phi'}$. This quantity is actually an unbiased estimate of the advantage function:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbb{E}_\\tau \\left[ \\delta_t^{\\pi_\\theta} \\mid s_t, a_t \\right] &= \\mathbb{E}_\\tau \\left[ r_t + \\gamma V^{\\pi_\\theta}(s_{t+1}) - V^{\\pi_\\theta}(s_t) \\mid s_t, a_t \\right] \\\\\n",
    "&= \\mathbb{E}_\\tau \\left[ r_t + \\gamma V^{\\pi_\\theta}(s_{t+1}) \\mid s_t, a_t \\right] - V^{\\pi_\\theta}(s_t) \\\\\n",
    "&= Q^{\\pi_\\theta}(s_t, a_t) - V^{\\pi_\\theta}(s_t) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Using our neural network $V_{\\phi'}$, define\n",
    "\n",
    "$$\n",
    "\\delta_t = r_t + \\gamma V_{\\phi'}(s_{t+1}) - V_{\\phi'}(s_t)\n",
    "$$\n",
    "\n",
    "the empirical TD-error. Then we have another estimate for our advantage:\n",
    "\n",
    "$$\n",
    "\\hat{A}(s_t) = \\delta_t\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalized Advantage Estimation\n",
    "\n",
    "So far, we have seen many versions of the policy gradient theorem:\n",
    "\n",
    "$$\n",
    "g = \\mathbb{E}_\\tau \\left[ \\sum_{t=0}^T \\Psi_t \\nabla_\\theta \\log \\pi_\\theta (a_t \\mid s_t) \\right]\n",
    "$$\n",
    "\n",
    "where $\\Psi_t$ is one of:\n",
    "\n",
    "- $R_t = \\sum_{k=t}^T r_k$\n",
    "- $G_t = \\sum_{k=t}^T \\gamma^{k-t} r_k$\n",
    "- $Q^{\\pi_\\theta}(s_t)$\n",
    "- $A^{\\pi_\\theta}(s_t, a_t) = Q^{\\pi_\\theta}(s_t, a_t) - V^{\\pi_\\theta}(s_t)$\n",
    "- $\\delta_t^{\\pi_\\theta} = r_t + \\gamma V^{\\pi_\\theta}(s_{t+1}) - V^{\\pi_\\theta}(s_t)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can expand the Bellman equation for the value function for $n$ time steps as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "V^{\\pi_\\theta}(s_t) &= \\mathbb{E}_\\tau \\left[ G_t \\mid s_t \\right] \\\\\n",
    "&= \\mathbb{E}_\\tau \\left[ r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + \\dots \\mid s_t \\right] \\\\\n",
    "&= \\mathbb{E}_\\tau \\left[ r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + \\dots + \\gamma^n V^{\\pi_\\theta}(s_{t+n}) \\mid s_t \\right] \\\\\n",
    "&= \\mathbb{E}_\\tau \\left[ \\sum_{k=t}^{n-1} \\gamma^{k-t} r_{k} + \\gamma^n V^{\\pi_\\theta}(s_{t+n}) \\mid s_t \\right] \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Whereas before we only expanded it for a single time step, here we can expand it for $n$ time steps. We can then define the $n$-step TD-error as\n",
    "\n",
    "$$\n",
    "\\delta_t^n = - V^{\\pi_\\theta}(s_t) + \\sum_{k=t}^{n-1} \\gamma^{k-t} r_{k} + \\gamma^n V^{\\pi_\\theta}(s_{t+n}) \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is actually equivalent to a discounted sum of 1-step TD-errors:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\delta_t^n &= \\sum_{i=0}^n \\gamma^i \\delta_{t+i} \\\\\n",
    "&= \\delta_t + \\gamma \\delta_{t+1} + \\gamma^2 \\delta_{t+2} + \\dots \\\\\n",
    "&= \\quad \\ \\left( -V^{\\pi_\\theta}(s_t) + r_t + \\gamma V^{\\pi_\\theta}(s_{t+1}) \\right) \\\\\n",
    "& \\quad + \\gamma \\left( -V^{\\pi_\\theta}(s_{t+1}) + r_{t+2} + \\gamma^2 V^{\\pi_\\theta}(s_{t+2}) \\right) \\\\\n",
    "& \\quad + \\gamma^2 \\left( -V^{\\pi_\\theta}(s_{t+2}) + r_{t+3} + \\gamma^3 V^{\\pi_\\theta}(s_{t+3}) \\right) \\\\\n",
    "& \\quad + \\dots \\\\\n",
    "& \\quad + \\gamma^n \\left( -V^{\\pi_\\theta}(s_{t+n-1}) + r_{t+n} + \\gamma^n V^{\\pi_\\theta}(s_{t+n}) \\right) \\\\\n",
    "&= -V^{\\pi_\\theta}(s_t) + r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + \\gamma^3 r_{t+3} + \\dots + \\gamma^n V^{\\pi_\\theta}(s_{t+n}) \\\\\n",
    "&= -V^{\\pi_\\theta}(s_t) +\\sum_{k=t}^{n-1} \\gamma^{k-t} r_{k} + \\gamma^n V^{\\pi_\\theta}(s_{t+n})\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since in adjacent terms, the value function cancels out, leaving us with the $n$-step TD error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like we can use the 1-step TD error as an advantage estimate $\\hat{A}(s_t)$, we can likewise use the $n$-step TD error as an advantage estimate $\\hat{A}^{(n)}(s_t)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generalized advantage estimation is just an exponentially weighted average of these $n$-step estimators.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{A}^{\\text{GAE}(\\gamma, \\lambda)} &= (1 - \\lambda) \\left( \\hat{A}^{(1)}(s_t) + \\lambda \\hat{A}^{(2)}(s_{t+1}) + \\lambda^2 \\hat{A}^{(3)}(s_t) + \\dots \\right) \\\\\n",
    "&= (1 - \\lambda) \\left( \\delta_t^{\\pi_\\theta} + \\lambda \\left(\\delta_t^{\\pi_\\theta} + \\gamma \\delta_{t+1}^{\\pi_\\theta} \\right) + \\lambda^2 \\left( \\delta_t^{\\pi_\\theta} + \\gamma \\delta_{t+1}^{\\pi_\\theta} + \\gamma^2 \\delta_{t+2}^{\\pi_\\theta} \\right) + \\dots \\right) \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expanding and refactoring, we get\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\hat{A}^{\\text{GAE}(\\gamma, \\lambda)}}{1 - \\lambda} &= \n",
    " \\delta_t^{\\pi_\\theta} \\left(1 + \\lambda + \\lambda^2 + \\dots \\right) \\\\\n",
    "& + \\delta_{t+1}^{\\pi_\\theta} \\left( \\lambda + \\lambda^2 + \\lambda^3 + \\dots \\right) \\\\\n",
    "& + \\delta_{t+2}^{\\pi_\\theta} \\left( \\lambda^2 + \\lambda^3 + \\lambda^3 + \\dots \\right) + \\dots \n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just an infinite geometric series, yielding\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\hat{A}^{\\text{GAE}(\\gamma, \\lambda)}}{1 - \\lambda} &= \\delta_t^{\\pi_\\theta} \\left( \\frac{1}{1-\\lambda} \\right) + \\delta_{t+1}^{\\pi_\\theta} \\left( \\frac{\\lambda}{1-\\lambda} \\right) + \\delta_{t+2}^{\\pi_\\theta} \\left( \\frac{\\lambda^2}{1-\\lambda} \\right) + \\dots \\\\\n",
    "\\hat{A}^{\\text{GAE}(\\gamma, \\lambda)} &= \\sum_{i=0}^\\infty (\\gamma \\lambda)^i \\delta_{t+i}^{\\pi_\\theta}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that the exponentially weighted sum has a nice and simple expression that relies only on single-step TD errors. This is good for us in terms of implementation because we can use the TD errors to fit the value network $V_{\\phi'}$ and then we can reuse them the get an advantage estimate using GAE.\n",
    "\n",
    "There are two special cases pointed out in the [original paper](https://arxiv.org/pdf/1506.02438.pdf). When $\\lambda=0$, we get\n",
    "\n",
    "$$\n",
    "\\hat{A}^{\\text{GAE}(\\gamma, 0)} = \\delta_t^{\\pi_\\theta}\n",
    "$$\n",
    "\n",
    "when $\\lambda=1$ we get\n",
    "\n",
    "$$\n",
    "\\hat{A}^{\\text{GAE}(\\gamma, 0)} = \\sum_{i=0}^\\infty \\gamma^i \\delta_{t+i}^{\\pi_\\theta} = -V^{\\pi_\\theta}(s_t) + \\sum_{i=0}^\\infty \\gamma^i r_{t+i} = -V^{\\pi_\\theta}(s_t) + G_t\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So $\\lambda$ can let us interpolate between using the TD error as an advantage function, and using the empirical returns minus $V^{\\pi)\\theta}(s_t)$, both of which we covered above as possible advantage estimates. \n",
    "\n",
    "By using a high value of $\\lambda$, we use a larger sum of terms by discounting less in the future. This gives us a less biased estimate of the advantage, but is more variable. Likewise, a low value of $\\lambda$ uses a smaller sum of terms by discounting more in the future. This gives us a more biased estimate but is less variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantage Actor-Critic (A2C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An **actor-critic** algorithm is a policy gradient algorithm that uses function estimation in place of empirical returns $G_t$ in the policy gradient update. The **actor** is the policy $\\pi_\\theta$ and the **critic** is usually a value function $V_\\phi$. The actor is trained using gradient ascent on the policy gradient, and the critic is trained via regression. The regression target can either be the empirical returns $G_t$, or can be the Bellman target $r_t + \\gamma V_\\phi(s_t)$.\n",
    "\n",
    "An **advantage actor-critic** is just an actor-critic that uses the **advantage** $\\hat{A}(s_t)$ instead of $V^{\\pi_\\theta}$. Advantage can be estimated using any of the means described above.\n",
    "\n",
    "For our purposes, we will use GAE for our advantage estimate, and fit our $V_\\phi$ network using the empirical returns as the targets. We have a fixed rollout length $T$ that we use that makes every environment episodic by stopping after $T$ time steps. We can bootstrap what the \"rest of the rewards\" would have been by adding to the final reward $r_T$ the predicted value:\n",
    "\n",
    "$$\n",
    "r_T \\gets r_T + \\gamma (1-d_T) V_\\phi(s_{T+1})\n",
    "$$\n",
    "\n",
    "where $d_T$ is the terminal flag for the penultimate state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we use the exact same code for vectorizing our environments, as well as for our two policies. The only change to the policies is semantic in that we change the `returns` to `advantages` in the `learn` method of the policy (though from a code perspective they are the same)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_device(device=''):\n",
    "    if device.lower() == 'cuda':\n",
    "        if not torch.cuda.is_available():\n",
    "            print (\"torch.cuda not available\")\n",
    "            return torch.device('cpu')    \n",
    "        else:\n",
    "            return torch.device('cuda:0')\n",
    "    if device.lower() == 'dml':\n",
    "        return torch.device('dml')\n",
    "    else:\n",
    "        return torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "class VectorizedEnvWrapper(gym.Wrapper):\n",
    "    def __init__(self, env, num_envs=1):\n",
    "        '''\n",
    "        env (gym.Env): to make copies of\n",
    "        num_envs (int): number of copies\n",
    "        '''\n",
    "        super().__init__(env)\n",
    "        self.num_envs = num_envs\n",
    "        self.envs = [copy.deepcopy(env) for n in range(num_envs)]\n",
    "    \n",
    "    def reset(self):\n",
    "        '''\n",
    "        Return and reset each environment\n",
    "        '''\n",
    "        return np.asarray([env.reset() for env in self.envs])\n",
    "    \n",
    "    def step(self, actions):\n",
    "        '''\n",
    "        Take a step in the environment and return the result.\n",
    "        actions (torch.tensor)\n",
    "        '''\n",
    "        next_states, rewards, dones = [], [], []\n",
    "        for env, action in zip(self.envs, actions):\n",
    "            next_state, reward, done, _ = env.step(action.item())\n",
    "            if done:\n",
    "                next_states.append(env.reset())\n",
    "            else:\n",
    "                next_states.append(next_state)\n",
    "            rewards.append(reward)\n",
    "            dones.append(done)\n",
    "        return np.asarray(next_states), np.asarray(rewards), \\\n",
    "            np.asarray(dones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Policy:\n",
    "    def pi(self, s_t):\n",
    "        '''\n",
    "        returns the probability distribution over actions \n",
    "        (torch.distributions.Distribution)\n",
    "        \n",
    "        s_t (np.ndarray): the current state\n",
    "        '''\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def act(self, s_t):\n",
    "        '''\n",
    "        s_t (np.ndarray): the current state\n",
    "        Because of environment vectorization, this will produce\n",
    "        E actions where E is the number of parallel environments.\n",
    "        '''\n",
    "        a_t = self.pi(s_t).sample()\n",
    "        return a_t\n",
    "    \n",
    "    def learn(self, states, actions, advantages):\n",
    "        '''\n",
    "        states (np.ndarray): the list of states encountered during\n",
    "                             rollout\n",
    "        actions (np.ndarray): the list of actions encountered during\n",
    "                              rollout\n",
    "        advantages (np.ndarray): the list of advantages encountered during\n",
    "                              rollout\n",
    "        \n",
    "        Because of environment vectorization, each of these has first\n",
    "        two dimensions TxE where T is the number of time steps in the\n",
    "        rollout and E is the number of parallel environments.\n",
    "        '''\n",
    "        actions = torch.tensor(actions)\n",
    "        advantages = torch.tensor(advantages)\n",
    "\n",
    "        log_prob = self.pi(states).log_prob(actions)\n",
    "        loss = torch.mean(-log_prob*advantages)\n",
    "        self.opt.zero_grad()\n",
    "        loss.backward()\n",
    "        self.opt.step()\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, h, w, outputs):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "\n",
    "        # Number of Linear input connections depends on output of conv2d layers\n",
    "        # and therefore the input image size, so compute it.\n",
    "        def conv2d_size_out(size, kernel_size = 5, stride = 2):\n",
    "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
    "        print(convh, convw)\n",
    "        linear_input_size = convw * convh * 32\n",
    "        # self.head = nn.Linear(linear_input_size, outputs)\n",
    "        self.head = nn.Linear(linear_input_size, convw*convh)\n",
    "        # self.mid = nn.Linear(convw*convh/3, convh/2)\n",
    "        self.out = nn.Linear(convw*convh, outputs)\n",
    "        # self.out = F.softmax\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        # print(x)\n",
    "        # x = x.to(device)\n",
    "        # print('U', x[0])\n",
    "        # print('O', np.shape(x))\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        # return self.head(x.view(x.size(0), -1))\n",
    "        x = F.relu(self.head(x.view(x.size(0), -1)))\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import InterpolationMode\n",
    "import torchvision.transforms as T\n",
    "resize = T.Compose([T.ToPILImage(),\n",
    "                    T.Resize(40, interpolation=InterpolationMode.BICUBIC),\n",
    "                    T.ToTensor()])\n",
    "def get_cart_location(env, screen_width):\n",
    "    world_width = env.x_threshold * 2\n",
    "    scale = screen_width / world_width\n",
    "    return int(env.state[0] * scale + screen_width / 2.0)  # MIDDLE OF CART\n",
    "import time\n",
    "def get_screen(env):\n",
    "    # Returned screen requested by gym is 400x600x3, but is sometimes larger\n",
    "    # such as 800x1200x3. Transpose it into torch order (CHW).\n",
    "    # time.sleep(2)\n",
    "    screen = env.render(mode='rgb_array')\n",
    "    screen  = screen.transpose((2, 0, 1))\n",
    "    # screen = env.render(mode='rgb_array').transpose((2, 0, 1))\n",
    "    # Cart is in the lower half, so strip off the top and bottom of the screen\n",
    "    _, screen_height, screen_width = screen.shape\n",
    "    screen = screen[:, int(screen_height*0.4):int(screen_height * 0.8)]\n",
    "    view_width = int(screen_width * 0.6)\n",
    "    cart_location = get_cart_location(env, screen_width)\n",
    "    if cart_location < view_width // 2:\n",
    "        slice_range = slice(view_width)\n",
    "    elif cart_location > (screen_width - view_width // 2):\n",
    "        slice_range = slice(-view_width, None)\n",
    "    else:\n",
    "        slice_range = slice(cart_location - view_width // 2,\n",
    "                            cart_location + view_width // 2)\n",
    "    # Strip off the edges, so that we have a square image centered on a cart\n",
    "    screen = screen[:, :, slice_range]\n",
    "    # Convert to float, rescale, convert to torch tensor\n",
    "    # (this doesn't require a copy)\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    screen = torch.from_numpy(screen)\n",
    "    # Resize, and add a batch dimension (BCHW)\n",
    "    return resize(screen).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiagonalGaussianPolicy2(Policy):\n",
    "    def __init__(self, env, lr=1e-2):\n",
    "        '''\n",
    "        env (gym.Env): the environment\n",
    "        lr (float): learning rate\n",
    "        '''\n",
    "        self.N = env.observation_space.shape[0]\n",
    "        self.M = env.action_space.shape[0]\n",
    "        _, _, h, w = get_screen(env.envs[0]).shape\n",
    "        def conv2d_size_out(size, kernel_size = 5, stride = 2):\n",
    "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
    "        linear_input_size = convw * convh * 32\n",
    "        self.mu = torch.nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=5, stride=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=5, stride=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(linear_input_size, convw*convh),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(convw*convh, self.M)\n",
    "        ).double()\n",
    "        \n",
    "        self.log_sigma = torch.ones(self.M, dtype=torch.double, requires_grad=True)\n",
    "\n",
    "        self.opt = torch.optim.Adam(list(self.mu.parameters()) + [self.log_sigma], lr=lr)\n",
    "        \n",
    "    def pi(self, s_t):\n",
    "        '''\n",
    "        returns the probability distribution over actions\n",
    "        s_t (np.ndarray): the current state\n",
    "        '''\n",
    "        s_t = torch.as_tensor(s_t).double()\n",
    "        mu = self.mu(s_t)\n",
    "        log_sigma = self.log_sigma\n",
    "        sigma = torch.exp(log_sigma)\n",
    "        pi = torch.distributions.MultivariateNormal(mu, torch.diag(sigma))\n",
    "        return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoricalPolicy(Policy):\n",
    "    def __init__(self, env, lr=1e-2):\n",
    "        '''\n",
    "        env (gym.Env): the environment\n",
    "        lr (float): learning rate\n",
    "        '''\n",
    "        self.N = env.observation_space.shape[0]\n",
    "        self.M = env.action_space.n\n",
    "        _, _, h, w = get_screen(env.envs[0]).shape\n",
    "        def conv2d_size_out(size, kernel_size = 5, stride = 2):\n",
    "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
    "        linear_input_size = convw * convh * 32\n",
    "        self.p = torch.nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=5, stride=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=5, stride=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(linear_input_size, convw*convh),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(convw*convh, self.M)\n",
    "        ).double()\n",
    "        \n",
    "        self.opt = torch.optim.Adam(self.p.parameters(), lr=lr)\n",
    "        \n",
    "    def pi(self, s_t):\n",
    "        '''\n",
    "        returns the probability distribution over actions\n",
    "        s_t (np.ndarray): the current state\n",
    "        '''\n",
    "        s_t = torch.as_tensor(s_t).double()\n",
    "        p = self.p(s_t)\n",
    "        pi = torch.distributions.Categorical(logits=p)\n",
    "        return pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we implement a `ValueEstimator` class. It is a small neural network that predicts the value function $V_\\phi(s_t)$. It is fit using regression to predict the returns $G_t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_returns(rewards, dones, gamma):\n",
    "    result = np.empty_like(rewards)\n",
    "    result[-1] = rewards[-1]\n",
    "    for t in range(len(rewards)-2, -1, -1):\n",
    "        result[t] = rewards[t] + gamma*(1-dones[t])*result[t+1]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_advantages(TD_errors, lam, gamma):\n",
    "    result = np.empty_like(TD_errors)\n",
    "    result[-1] = TD_errors[-1]\n",
    "    for t in range(len(TD_errors)-2, -1, -1):\n",
    "        result[t] = TD_errors[t] + gamma*lam*result[t+1]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueEstimator:\n",
    "    def __init__(self, env, lr=1e-2):\n",
    "        self.N = env.observation_space.shape[0]\n",
    "        _, _, h, w = get_screen(env.envs[0]).shape\n",
    "        def conv2d_size_out(size, kernel_size = 5, stride = 2):\n",
    "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
    "        linear_input_size = convw * convh * 32\n",
    "        self.V = torch.nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=5, stride=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=5, stride=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(linear_input_size, convw*convh),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(convw*convh, 1)\n",
    "        ).double()\n",
    "\n",
    "        self.opt = torch.optim.Adam(self.V.parameters(), lr=lr)\n",
    "        \n",
    "    def predict(self, s_t):\n",
    "        s_t = torch.tensor(s_t)\n",
    "        return self.V(s_t).squeeze()\n",
    "\n",
    "    def learn(self, V_pred, returns):\n",
    "        self.opt.zero_grad()\n",
    "        returns = torch.tensor(returns)\n",
    "        loss = torch.mean((V_pred - returns)**2)\n",
    "        loss2 = loss\n",
    "        # print('K', loss2)\n",
    "        # self.opt.zero_grad()\n",
    "        loss.backward()\n",
    "        print('K', loss)\n",
    "        self.opt.step()\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns; sns.set()\n",
    "import time\n",
    "\n",
    "def A2C(env, agent, value_estimator, \n",
    "        gamma=0.99, lam=0.95, \n",
    "        epochs=256, train_V_iters=80, T=200):    \n",
    "    states = np.empty((T+1, env.num_envs, agent.N))\n",
    "    _, _, h, w = get_screen(env.envs[0]).shape\n",
    "    frames = np.empty((T+1, env.num_envs, 3, h, w))\n",
    "    frames_t = np.empty((env.num_envs, 3, h, w))\n",
    "    V_pred = torch.empty((T+1, env.num_envs))\n",
    "    v_opt = value_estimator.opt\n",
    "    value_estimator_ = value_estimator\n",
    "    temp = 0\n",
    "    if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "        actions = np.empty((T, env.num_envs))\n",
    "    else:\n",
    "        actions = np.empty((T, env.num_envs, agent.M))\n",
    "    rewards = np.empty((T, env.num_envs))\n",
    "    rewards_ = np.empty((T, env.num_envs))\n",
    "    dones = np.empty((T, env.num_envs))\n",
    "    \n",
    "    totals = []\n",
    "    \n",
    "    s_t = env.reset()\n",
    "    for (i, en) in enumerate(env.envs):\n",
    "        frames_t[i] = get_screen(en).squeeze()\n",
    "    for epoch in range(epochs):\n",
    "        for t in range(T):\n",
    "            a_t = agent.act(frames_t)\n",
    "            s_t_next, r_t, d_t = env.step(a_t)\n",
    "            frames[t] = frames_t\n",
    "            states[t] = s_t\n",
    "            actions[t] = a_t\n",
    "            rewards[t] = r_t\n",
    "            dones[t] = d_t\n",
    "            s_t = s_t_next\n",
    "            for (i, en) in enumerate(env.envs):\n",
    "                frames_t[i] = get_screen(en).squeeze()\n",
    "            # env.envs[0].render()\n",
    "            # env.envs[3].render()\n",
    "            # env.envs[7].render()\n",
    "        states[T] = s_t\n",
    "\n",
    "        # bootstrap\n",
    "        V_last = value_estimator.predict(frames[-1]).detach().numpy() \n",
    "        rewards[-1] += gamma*(1-dones[-1])*V_last\n",
    "        returns = calculate_returns(rewards, dones, gamma)\n",
    "\n",
    "        # V_last_ = value_estimator_.predict(states[-1]).detach().numpy() \n",
    "        # rewards_[-1] += gamma*(1-dones[-1])*V_last_\n",
    "        # returns_ = calculate_returns(rewards_, dones, gamma)\n",
    "        # returns = torch.tensor(returns)\n",
    "        # temp = torch.mean((V_pred[:-1] - returns)**2)\n",
    "\n",
    "        for i in range(train_V_iters):\n",
    "            V_pred = value_estimator.predict(frames)\n",
    "            # for (j, f) in enumerate(frames):\n",
    "            #     V_pred[j] = value_estimator.predict(frames[j])\n",
    "            print('Y', V_pred)\n",
    "            # value_estimator_.learn(V_pred_[:-1], returns_)\n",
    "            value_estimator.learn(V_pred[:-1], returns)\n",
    "            # v_opt.zero_grad()\n",
    "            # returns = torch.tensor(returns)\n",
    "            # loss = torch.mean((V_pred[:-1] - returns)**2)\n",
    "            # loss.backward()\n",
    "            # v_opt.step()\n",
    "\n",
    "        \n",
    "        # compute advantages\n",
    "        V_pred = V_pred.detach().numpy()\n",
    "        TD_errors = rewards + gamma*(1-dones)*V_pred[1:] - V_pred[:-1]\n",
    "        advantages = calculate_advantages(TD_errors, lam, gamma)\n",
    "        \n",
    "        # normalize advantages\n",
    "        advantages = (advantages - advantages.mean())/advantages.std()\n",
    "        \n",
    "        pi_loss = agent.learn(frames[:-1], actions, advantages)\n",
    "        \n",
    "        totals.append(rewards.sum()/dones.sum())\n",
    "        print(f'{epoch}/{epochs}:{totals[-1]}\\r', end='')\n",
    "        \n",
    "    sns.lineplot(x=range(len(totals)), y=totals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.03540917  0.04716156  0.00141694  0.00485552]\n",
      " [-0.03540917  0.04716156  0.00141694  0.00485552]]\n",
      "-0.03540917152522845\n"
     ]
    }
   ],
   "source": [
    "cartpole = VectorizedEnvWrapper(gym.make(\"CartPole-v0\"), num_envs=2)\n",
    "s = cartpole.reset()\n",
    "print(s)\n",
    "print(cartpole.envs[0].state[0])\n",
    "# np.array(get_screen(env=cartpole.envs[0])).squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected 4-dimensional input for 4-dimensional weight [16, 3, 5, 5], but got 5-dimensional input of size [201, 2, 3, 40, 90] instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1956/2032691301.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcategorical\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCategoricalPolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcartpole\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mvalue_estimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mValueEstimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcartpole\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mA2C\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcartpole\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategorical\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_estimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_1956/1930824422.py\u001b[0m in \u001b[0;36mA2C\u001b[0;34m(env, agent, value_estimator, gamma, lam, epochs, train_V_iters, T)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_V_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0mV_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m             \u001b[0;31m# for (j, f) in enumerate(frames):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0;31m#     V_pred[j] = value_estimator.predict(frames[j])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1956/1045597358.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, s_t)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0ms_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/RL/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/RL/lib/python3.9/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/RL/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/RL/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/RL/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    440\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 442\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    443\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 4-dimensional input for 4-dimensional weight [16, 3, 5, 5], but got 5-dimensional input of size [201, 2, 3, 40, 90] instead"
     ]
    }
   ],
   "source": [
    "categorical = CategoricalPolicy(cartpole, lr=1e-1)\n",
    "value_estimator = ValueEstimator(cartpole, lr=1e-2)\n",
    "A2C(cartpole, categorical, value_estimator, epochs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we run this algorithm on the `CartPoleSwingUp` environment, which as we discussed in the previous post, is a continuous environment. This environment has a much longer time horizon than `CartPole-v0`, so we increase $\\gamma$ to 0.999. We also use a large value of $\\lambda$ (0.99 versus 0.95 for cartpole) to get a less biased estimate of the advantage.\n",
    "\n",
    "Hyperparameter tuning can be difficult for RL, where some algorithms' performance depends significantly on the learning rates of the policy and value networks, and on $\\lambda$, $\\gamma$, and even the rollout lenth $T$. To find these hyperparameters, I ran a **grid search**. To perform a grid search, create a list of potential values for each hyperparameter that you want to tune. Then, take every possible combination of these hyperparameters and run them on 5 random seeds. Look at the overall performance of each seed to pick the best set of hyperparameters. \n",
    "\n",
    "The usual recipe that I follow is something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "policy_lrs = [0.1, 0.01, 0.001]\n",
    "value_lrs = [0.1, 0.01, 0.001]\n",
    "gammas = [0.99, 0.999]\n",
    "lambdas = [0.9, 0.99]\n",
    "\n",
    "hyperparameters = itertools.product(\n",
    "                    policy_lrs,\n",
    "                    value_lrs,\n",
    "                    gammas,\n",
    "                    lambdas)\n",
    "\n",
    "for seed in [9827391, 8534101, 4305430, 12654329, 3483055]:\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    for policy_lr, value_lr, gamma, lam in hyperparameters:\n",
    "        pass\n",
    "        # run code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes researchers will run an \"initial\" grid search to get a ballpark for the approximate orders of magnitude that work well with learning, then do a second grid search with a finer mesh to try to find slightly better performing hyperparameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to run your code many times for different random seeds. Using the same seeds can be good for reproducibility. The truth is that many deep RL algorithms are brittle, which is why authors often report [aggregate measures](https://people.cs.umass.edu/~kclary/NeurIPS_Critiquing_Trends_Workshop_2018___Variability_of_Deep_RL.pdf) of their agents, like median or mean return over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I ran the code below with dozens of random seeds in parallel overnight, and then plotted the learning curves of each agent. The exact same hyperparameters were used - the only change was the random seed:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![swingup.png](swingup.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some agents get trapped at a local minimum around -30, which seems to be a policy that just accelerates sideways quickly to the edge of the environment to get some rewards. Some agents learn smoothly and converge around 100-200 over time. Some agents perform extremely well and end up getting up to 400 total rewards which is the environment maximum. From the paper linked above:\n",
    "\n",
    "> An agent with poorly seeded weights, or poorly chosen random actions early in training, may find its way to a local optimum and never achieve high reward. With this context in mind, deep RL researchers often train multiple agents with different random seeds to account for this variability in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/30:-28.259278247592304\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8213/2032223717.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdigauss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDiagonalGaussianPolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mswingup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mvalue_estimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mValueEstimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mswingup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mA2C\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mswingup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdigauss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_estimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.999\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.99\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4052\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_8213/3369246136.py\u001b[0m in \u001b[0;36mA2C\u001b[0;34m(env, agent, value_estimator, gamma, lam, epochs, train_V_iters, T)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_V_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0mV_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0mvalue_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_8213/3515771027.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, s_t)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0ms_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/RL/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/RL/lib/python3.9/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/RL/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/RL/lib/python3.9/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/RL/lib/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gym_cartpole_swingup\n",
    "swingup = VectorizedEnvWrapper(gym.make(\"CartPoleSwingUp-v0\"), num_envs=8)\n",
    "digauss = DiagonalGaussianPolicy(swingup, lr=1e-2)\n",
    "value_estimator = ValueEstimator(swingup, lr=1e-1)\n",
    "A2C(swingup, digauss, value_estimator, gamma=0.999, lam=0.99, epochs=30, T=4052)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like this particular agent is learning well, and increasing the number of learning epochs may have lead to even better performance down the road!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous post, I just showed the learning curves of the two agents. I included those here as well, but I thought it would fun to record a GIF of the agent applying the learned policy in the environment. The code below runs a trained agent and produces a GIF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "from PIL import Image\n",
    "\n",
    "def generate_gif(env, agent, filename, T=200):\n",
    "    frames = []\n",
    "    s_t = env.reset()\n",
    "    for t in range(T):\n",
    "        a_t = agent.act(s_t)\n",
    "        s_t, r_t, d_t = env.step(a_t)\n",
    "        frame = env.envs[0].render(mode='rgb_array')\n",
    "        frames.append(frame)\n",
    "\n",
    "    images_list = [Image.fromarray(frame) for frame in frames]\n",
    "    imageio.mimsave(f'{filename}.gif', frames, duration=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_gif(cartpole, categorical, 'cartpole')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![cartpole](cartpole.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_gif(swingup, digauss, 'swingup')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![swingup](swingup.gif)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
