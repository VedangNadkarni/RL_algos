{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym.spaces import Discrete, Box\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as optimizer\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib\n",
    "import torchvision.transforms as T\n",
    "from torch.distributions import Categorical, Normal\n",
    "from scipy import signal\n",
    "import scipy\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# envfunv = lambda : gym.make('CartPole-v0')\n",
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# env2 = envfunv()\n",
    "env.action_space\n",
    "# env2.reset()\n",
    "# env2.step(1)\n",
    "# env.step(1)\n",
    "# print(type(env), type(env2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, h, w, outputs):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "\n",
    "        # Number of Linear input connections depends on output of conv2d layers\n",
    "        # and therefore the input image size, so compute it.\n",
    "        def conv2d_size_out(size, kernel_size = 5, stride = 2):\n",
    "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
    "        linear_input_size = convw * convh * 32\n",
    "        self.head = nn.Linear(linear_input_size, outputs)\n",
    "        # self.out = F.softmax\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        # print(x)\n",
    "        x = x.to(device)\n",
    "        # print('U', x[0])\n",
    "        # print('O', np.shape(x))\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        return self.head(x.view(x.size(0), -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import InterpolationMode\n",
    "resize = T.Compose([T.ToPILImage(),\n",
    "                    T.Resize(40, interpolation=InterpolationMode.BICUBIC),\n",
    "                    T.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cart_location(screen_width):\n",
    "    world_width = env.x_threshold * 2\n",
    "    scale = screen_width / world_width\n",
    "    return int(env.state[0] * scale + screen_width / 2.0)  # MIDDLE OF CART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def get_screen():\n",
    "    # Returned screen requested by gym is 400x600x3, but is sometimes larger\n",
    "    # such as 800x1200x3. Transpose it into torch order (CHW).\n",
    "    # time.sleep(2)\n",
    "    screen = env.render(mode='rgb_array')\n",
    "    screen  = screen.transpose((2, 0, 1))\n",
    "    # screen = env.render(mode='rgb_array').transpose((2, 0, 1))\n",
    "    # Cart is in the lower half, so strip off the top and bottom of the screen\n",
    "    _, screen_height, screen_width = screen.shape\n",
    "    screen = screen[:, int(screen_height*0.4):int(screen_height * 0.8)]\n",
    "    view_width = int(screen_width * 0.6)\n",
    "    cart_location = get_cart_location(screen_width)\n",
    "    if cart_location < view_width // 2:\n",
    "        slice_range = slice(view_width)\n",
    "    elif cart_location > (screen_width - view_width // 2):\n",
    "        slice_range = slice(-view_width, None)\n",
    "    else:\n",
    "        slice_range = slice(cart_location - view_width // 2,\n",
    "                            cart_location + view_width // 2)\n",
    "    # Strip off the edges, so that we have a square image centered on a cart\n",
    "    screen = screen[:, :, slice_range]\n",
    "    # Convert to float, rescale, convert to torch tensor\n",
    "    # (this doesn't require a copy)\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    screen = torch.from_numpy(screen)\n",
    "    # Resize, and add a batch dimension (BCHW)\n",
    "    return resize(screen).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env.reset()\n",
    "# print(env.render(mode=\"rgb_array\"))\n",
    "plt.figure()\n",
    "plt.imshow(get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy(),\n",
    "           interpolation='none')\n",
    "plt.title('Example extracted screen')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    \n",
    "    def _distribution(self, obs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _log_prob_from_distribution(self, pi, act):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, obs, act=None):\n",
    "        # Produce action distributions for given observations, and \n",
    "        # optionally compute the log likelihood of given actions under\n",
    "        # those distributions.\n",
    "        pi = self._distribution(obs)\n",
    "        logp_a = None\n",
    "        if act is not None:\n",
    "            logp_a = self._log_prob_from_distribution(pi, act)\n",
    "        return pi, logp_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoricalActor(Actor):\n",
    "    def __init__(self, h, w, act_dim = 1):\n",
    "        super().__init__()\n",
    "        self.logits_net = Net(h, w, act_dim)\n",
    "\n",
    "    def _distribution(self, obs):\n",
    "        logits = self.logits_net(obs)\n",
    "        # print(logits)\n",
    "        return Categorical(logits=logits)\n",
    "\n",
    "    def _log_prob_from_distribution(self, pi, act):\n",
    "        return pi.log_prob(act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianActor(Actor):\n",
    "\n",
    "    def __init__(self, h, w, act_dim = 1):\n",
    "        super().__init__()\n",
    "        log_std = -0.5 * np.ones(act_dim, dtype=np.float32)\n",
    "        self.log_std = torch.nn.Parameter(torch.as_tensor(log_std))\n",
    "        self.mu_net = Net(h, w, env.action_space.n)\n",
    "\n",
    "    def _distribution(self, obs):\n",
    "        mu = self.mu_net(obs)\n",
    "        std = torch.exp(self.log_std)\n",
    "        return Normal(mu, std)\n",
    "\n",
    "    def _log_prob_from_distribution(self, pi, act):\n",
    "        return pi.log_prob(act).sum(axis=-1)    # Last axis sum needed for Torch Normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "\n",
    "    def __init__(self, h, w):\n",
    "        super().__init__()\n",
    "        self.v_net = Net(h, w, 1)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        return torch.squeeze(self.v_net(obs), -1) # Critical to ensure v has right shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(self, h, w, action_space):\n",
    "        super().__init__()\n",
    "\n",
    "        # policy builder depends on action space\n",
    "        if isinstance(action_space, Box):\n",
    "            self.pi = GaussianActor(h, w, action_space.n)\n",
    "        elif isinstance(action_space, Discrete):\n",
    "            self.pi = CategoricalActor(h, w, action_space.n)\n",
    "\n",
    "        # build value function\n",
    "        self.v  = Critic(h, w)\n",
    "\n",
    "    def step(self, obs):\n",
    "        with torch.no_grad():\n",
    "            pi = self.pi._distribution(obs)\n",
    "            a = pi.sample()\n",
    "            logp_a = self.pi._log_prob_from_distribution(pi, a)\n",
    "            v = self.v(obs)\n",
    "        return a.numpy(), v.numpy(), logp_a.numpy()\n",
    "\n",
    "    def act(self, obs):\n",
    "        return self.step(obs)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_cumsum(x, discount):\n",
    "    \"\"\"\n",
    "    magic from rllab for computing discounted cumulative sums of vectors.\n",
    "\n",
    "    input: \n",
    "        vector x, \n",
    "        [x0, \n",
    "         x1, \n",
    "         x2]\n",
    "\n",
    "    output:\n",
    "        [x0 + discount * x1 + discount^2 * x2,  \n",
    "         x1 + discount * x2,\n",
    "         x2]\n",
    "    \"\"\"\n",
    "    return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A 0\n",
      "Categorical(logits: torch.Size([1, 2])) tensor([-0.4705], grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "init_screen = get_screen()\n",
    "_, _, screen_height, screen_width = init_screen.shape\n",
    "gamma = 0.9999\n",
    "lamda = 0.95\n",
    "# a = MLPGaussianActor(screen_height, screen_width, 2)\n",
    "a = ActorCritic(screen_height, screen_width, env.action_space)\n",
    "# o, r, d, i = env.step(0)\n",
    "# print(a.step(get_screen()))\n",
    "print('A', a.act(get_screen())[0])\n",
    "al, bl = a.pi(get_screen(), torch.as_tensor(a.act(get_screen())[0]))\n",
    "print(al, bl)\n",
    "# print(a.logits_net(get_screen()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_screen = get_screen()\n",
    "_, _, screen_height, screen_width = init_screen.shape\n",
    "# pi = Net(screen_height, screen_width, env.action_space.n).to(device)\n",
    "# v = Net(screen_height, screen_width, 1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "tb = SummaryWriter()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tb.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# action_probs = ActorCritic(screen_height, screen_width, get_screen()).cpu().detach().numpy()\n",
    "# action_space = np.arange(env.action_space.n)\n",
    "# action = np.random.choice(action_space, p=action_probs)\n",
    "# print(action_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "epochs = 1000\n",
    "t = 200\n",
    "ac = ActorCritic(screen_height, screen_width, env.action_space).to(device)\n",
    "# pi_opt = optim.Adam(ac.pi.parameters(), lr = 10)\n",
    "pi_opt = optim.RMSprop(ac.pi.parameters(), lr=0.05)\n",
    "# v_opt = optim.Adam(ac.v.parameters(), lr = 10)\n",
    "v_opt = optim.RMSprop(ac.v.parameters(), lr = 0.05)\n",
    "train_v_iters = 80\n",
    "count = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tb.add_graph(ac.pi., get_screen())\n",
    "# tb.add_graph(ac.v, get_screen())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdeb163d44114c43811f0767ffd51bb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b38ef941caf54ee18b2a7ae548a3319e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vedang/miniconda3/envs/RL/lib/python3.9/site-packages/numpy/core/_methods.py:163: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asanyarray(a)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (17,1) (10,1) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_30868/728492554.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;31m#get()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m     \u001b[0madvantages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0madvantages\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madvantages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madvantages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m     \u001b[0madvantages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madvantages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0mreturns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mmean\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/RL/lib/python3.9/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m   3438\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3440\u001b[0;31m     return _methods._mean(a, axis=axis, dtype=dtype,\n\u001b[0m\u001b[1;32m   3441\u001b[0m                           out=out, **kwargs)\n\u001b[1;32m   3442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/RL/lib/python3.9/site-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0mis_float16_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         ret = um.true_divide(\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (17,1) (10,1) "
     ]
    }
   ],
   "source": [
    "gamma = 0.99\n",
    "lamda = 0.95\n",
    "epochs = 50\n",
    "steps_per_epoch = int(5e2)\n",
    "for ep in tqdm(range(0,epochs)):\n",
    "    \n",
    "    s = env.reset()\n",
    "    frames = []\n",
    "    ep_len = 1\n",
    "    # frames  =[]\n",
    "    states = []\n",
    "    rewards = []\n",
    "    actions = []\n",
    "    values = []\n",
    "    advantages = []\n",
    "    # logps = []\n",
    "    returns = []\n",
    "    done = False\n",
    "    timeout = 0\n",
    "    net_loss = 0\n",
    "    net_reward = 0\n",
    "    run_rew = 0\n",
    "    ptr = 0\n",
    "    # print('R', done, timeout, t, (not done) and (timeout<t))\n",
    "    frame = get_screen()\n",
    "    env.render()\n",
    "    # time.sleep(0.0001)\n",
    "    a, v, logp = ac.step(frame)\n",
    "    # action_probs = action_probs = pi(get_screen()).squeeze().cpu().detach().numpy()\n",
    "    # a = np.random.choice(action_space, p = action_probs)\n",
    "    # print('L', a[0])\n",
    "    # help(env.step(a))\n",
    "    s_, r, done, info = env.step(a[0])\n",
    "    states.append(s)\n",
    "    # print(frame)\n",
    "    # print('P', np.shape(frame))\n",
    "    frames = frame\n",
    "    # print('K', frames)\n",
    "    # frames.append(frame)\n",
    "    rewards.append(r)\n",
    "    actions = a\n",
    "    # actions.append(a)\n",
    "    values.append(v)\n",
    "    net_reward += r\n",
    "    run_rew = run_rew*gamma + r\n",
    "    # logps.append(logp)\n",
    "    s = s_\n",
    "    # s = s_\n",
    "    timeout+=1\n",
    "    count+=1\n",
    "    tb.add_scalar(\"Net reward\", net_reward, count)\n",
    "    tb.add_scalar(\"Running reward\", run_rew, count)\n",
    "    for j in tqdm(range(steps_per_epoch)):\n",
    "        ep_len+=1\n",
    "        frame = get_screen()\n",
    "        env.render()\n",
    "        # time.sleep(0.0001)\n",
    "        a, v, logp = ac.step(frame)\n",
    "        # action_probs = action_probs = pi(get_screen()).squeeze().cpu().detach().numpy()\n",
    "        # a = np.random.choice(action_space, p = action_probs)\n",
    "        # print('L', a[0])\n",
    "        # help(env.step(a))\n",
    "        s_, r, done, info = env.step(a[0])\n",
    "        states.append(s)\n",
    "        # print(frame)\n",
    "        # print('P', np.shape(frame))\n",
    "        frames = np.append(frames, frame, axis = 0)\n",
    "        # print('K', frames)\n",
    "        # frames.append(frame)\n",
    "        rewards.append(r)\n",
    "        actions = np.append(actions, a)\n",
    "        # actions.append(a)\n",
    "        values.append(v)\n",
    "        net_reward += r\n",
    "        run_rew = run_rew*gamma + r\n",
    "        # logps.append(logp)\n",
    "        s = s_\n",
    "        # s = s_\n",
    "        timeout+=1\n",
    "        count+=1\n",
    "        tb.add_scalar(\"Net reward\", net_reward, count)\n",
    "        tb.add_scalar(\"Running reward\", run_rew, count)\n",
    "        if done or timeout>t:\n",
    "            deltas = [rewards[i]+gamma*values[i+1] - values[i] for i in range(ptr, values.__len__()-1)]\n",
    "            # rewards[:-1] + gamma * values[1:] - values[:-1]\n",
    "            advantages.append(discount_cumsum(deltas, gamma*lamda))\n",
    "            returns.append(discount_cumsum(rewards, gamma))\n",
    "            s, ep_ret, ep_len = env.reset(), 0, 0\n",
    "            ptr = j+1\n",
    "        \n",
    "    # frames = np.ndarray(frames)\n",
    "    # print(frames) \n",
    "    # frames = [frame.tolist() for frame in frames]\n",
    "    # print(frames) \n",
    "    # actions = np.ndarray(actions)\n",
    "    # print(ep, frames) \n",
    "    # print(done)\n",
    "    frames = torch.as_tensor(frames, dtype=torch.float32).to(device)\n",
    "\n",
    "    # print(frames) \n",
    "    actions = torch.as_tensor(actions, dtype=torch.float32).to(device)\n",
    "    #get()\n",
    "    advantages = (advantages - np.mean(advantages))/np.std(advantages)\n",
    "    advantages = torch.as_tensor(advantages, dtype=torch.float32).to(device)\n",
    "    returns = torch.as_tensor(returns.copy(), dtype=torch.float32).to(device)\n",
    "    \n",
    "    #update()\n",
    "    pi_opt.zero_grad()\n",
    "    _, logps = ac.pi(frames, actions)\n",
    "    # logps = logps.detach().numpy()\n",
    "    loss_pi = torch.negative(torch.multiply(logps, advantages)).to(device)\n",
    "    loss_pi = torch.mean(loss_pi).to(device)\n",
    "    print(loss_pi)\n",
    "    loss_pi.backward()\n",
    "    pi_opt.step()\n",
    "\n",
    "    for i in range(train_v_iters):\n",
    "        v_opt.zero_grad()\n",
    "        loss_v = torch.mean(torch.pow(torch.subtract((ac.v(frames)),returns),2)).to(device)\n",
    "        net_loss += loss_v.item()\n",
    "        loss_v.backward()\n",
    "        v_opt.step()\n",
    "        tb.add_scalar(\"loss_v\", loss_v.item(), i+ep*train_v_iters)\n",
    "    # print('Reward',net_reward, ep)\n",
    "    tb.add_scalar('Reward',net_reward, ep)\n",
    "    # print('Loss', net_loss, ep)\n",
    "    tb.add_scalar('Loss', net_loss, ep)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.]]]])]\n"
     ]
    }
   ],
   "source": [
    "print(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "from PIL import Image\n",
    "\n",
    "def generate_gif(env, filename, T=200):\n",
    "    frames = []\n",
    "    s_t = env.reset()\n",
    "    reward = 0\n",
    "    for t in range(T):\n",
    "        # a_t = agent.act(s_t)\n",
    "        frame = get_screen()\n",
    "        # action_probs = ac.pi(torch.FloatTensor(frame)).detach().numpy().squeeze()\n",
    "        a, v, logp = ac.step(frame)\n",
    "        # action_probs = model(torch.FloatTensor(frame)).detach().numpy().squeeze()\n",
    "        # print(action_probs)\n",
    "        # action_probs = [0.5, 0.5]\n",
    "        # action = np.random.choice(env.action_space, p=action_probs)\n",
    "        s_, r, done, info = env.step(a[0])\n",
    "        # s_t, r_t, d_t, _ = env.step(action)\n",
    "        frame_ = env.render(mode='rgb_array')\n",
    "        frames.append(frame_)\n",
    "        reward += r\n",
    "        if done:\n",
    "            break\n",
    "    print(reward)\n",
    "    images_list = [Image.fromarray(frame) for frame in frames]\n",
    "    imageio.mimsave(f'{filename}.gif', frames, duration=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "9.0\n"
     ]
    }
   ],
   "source": [
    "generate_gif(env, './Test4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d77854923f65db7c87e5b368fa3fd71bdf7a68487e1e3730c1972eb3f5d18515"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('RL')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
