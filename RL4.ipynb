{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym.spaces import Discrete, Box\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as opt\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib\n",
    "import torchvision.transforms as T\n",
    "from torch.distributions import Categorical, Normal\n",
    "from scipy import signal\n",
    "import scipy\n",
    "from torch import optim\n",
    "import core\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import imageio\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistics_scalar(x, with_min_and_max=False):\n",
    "    \"\"\"\n",
    "    Get mean/std and optional min/max of scalar x across MPI processes.\n",
    "\n",
    "    Args:\n",
    "        x: An array containing samples of the scalar to produce statistics\n",
    "            for.\n",
    "\n",
    "        with_min_and_max (bool): If true, return min and max of x in \n",
    "            addition to mean and std.\n",
    "    \"\"\"\n",
    "    x = np.array(x, dtype=np.float32)\n",
    "    global_sum, global_n = np.sum(x), len(x)\n",
    "    mean = global_sum / global_n\n",
    "\n",
    "    global_sum_sq = np.sum((x - mean)**2)\n",
    "    std = np.sqrt(global_sum_sq / global_n)  # compute global std\n",
    "\n",
    "    if with_min_and_max:\n",
    "        global_min =np.min(x) if len(x) > 0 else np.inf\n",
    "        global_max =np.max(x) if len(x) > 0 else -np.inf\n",
    "        return mean, std, global_min, global_max\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VPGBuffer:\n",
    "    def __init__(self, obs_dim, c, h, w, act_dim, size, gamma=0.99, lam=0.95):\n",
    "        self.obs_buf = np.zeros(core.combined_shape(size, obs_dim), dtype=np.float32)\n",
    "        self.frame_buf = np.zeros(core.combined_shape(size, (c, h, w)), dtype=np.float32)\n",
    "        self.obs_buf = np.zeros(core.combined_shape(size, obs_dim), dtype=np.float32)\n",
    "        self.act_buf = np.zeros(core.combined_shape(size, act_dim), dtype=np.float32)\n",
    "        self.adv_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.rew_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.ret_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.val_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.logp_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.gamma, self.lam = gamma, lam\n",
    "        self.ptr, self.path_start_idx, self.max_size = 0, 0, size\n",
    "\n",
    "    def store(self, frame, obs, act, rew, val, logp):\n",
    "        assert self.ptr < self.max_size     # buffer has to have room so you can store\n",
    "        self.obs_buf[self.ptr] = obs\n",
    "        self.frame_buf[self.ptr] = frame\n",
    "        self.act_buf[self.ptr] = act\n",
    "        self.rew_buf[self.ptr] = rew\n",
    "        self.val_buf[self.ptr] = val\n",
    "        self.logp_buf[self.ptr] = logp\n",
    "        self.ptr += 1\n",
    "\n",
    "    def finish_path(self, last_val=0):\n",
    "        path_slice = slice(self.path_start_idx, self.ptr)\n",
    "        rews = np.append(self.rew_buf[path_slice], last_val)\n",
    "        vals = np.append(self.val_buf[path_slice], last_val)\n",
    "        \n",
    "        # the next two lines implement GAE-Lambda advantage calculation\n",
    "        deltas = rews[:-1] + self.gamma * vals[1:] - vals[:-1]\n",
    "        self.adv_buf[path_slice] = core.discount_cumsum(deltas, self.gamma * self.lam)\n",
    "        \n",
    "        # the next line computes rewards-to-go, to be targets for the value function\n",
    "        self.ret_buf[path_slice] = core.discount_cumsum(rews, self.gamma)[:-1]\n",
    "        \n",
    "        self.path_start_idx = self.ptr\n",
    "\n",
    "    def get(self):\n",
    "        assert self.ptr == self.max_size    # buffer has to be full before you can get\n",
    "        self.ptr, self.path_start_idx = 0, 0\n",
    "        # the next two lines implement the advantage normalization trick\n",
    "        adv_mean, adv_std = statistics_scalar(self.adv_buf)\n",
    "        self.adv_buf = (self.adv_buf - adv_mean) / adv_std\n",
    "        data = dict(frames=self.frame_buf, obs=self.obs_buf, act=self.act_buf, ret=self.ret_buf,\n",
    "                    adv=self.adv_buf, logp=self.logp_buf)\n",
    "        return {k: torch.as_tensor(v, dtype=torch.float32) for k,v in data.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = 'Pendulum-v0'\n",
    "# env = gym.make('CartPole-v0')\n",
    "def vpg(env_fn, actor_critic=core.ActorCritic, ac_kwargs=dict(),  seed=0, \n",
    "        steps_per_epoch=1000, epochs=50, gamma=0.99, pi_lr=3e-4,\n",
    "        vf_lr=1e-3, train_v_iters=80, lam=0.97, max_ep_len=1000,\n",
    "        tb=SummaryWriter(), save_freq=10):\n",
    "\n",
    "    # Random seed\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Instantiate environment\n",
    "    env = env_fn()\n",
    "    obs_dim = env.observation_space.shape\n",
    "    act_dim = env.action_space.shape\n",
    "    env.reset()\n",
    "    env.render()\n",
    "    \n",
    "    def get_screen(env):                             \n",
    "        # Returned screen requested by gym is 400x600x3, but is sometimes larger\n",
    "        # such as 800x1200x3. Transpose it into torch order (CHW).\n",
    "        # time.sleep(2)\n",
    "        # env = env()\n",
    "        screen = env.render(mode='rgb_array')\n",
    "        screen  = screen.transpose((2, 0, 1))\n",
    "        # screen = env.render(mode='rgb_array').transpose((2, 0, 1))\n",
    "        # Cart is in the lower half, so strip off the top and bottom of the screen\n",
    "        # _, screen_height, screen_width = screen.shape\n",
    "        # screen = screen[:, int(screen_height*0):int(screen_height )]\n",
    "        # view_width = int(screen_width * 1)\n",
    "        # def get_cart_location(screen_width):\n",
    "        #     world_width = env.x_threshold * 2\n",
    "        #     scale = screen_width / world_width\n",
    "        #     return int(env.state[0] * scale + screen_width / 2.0)  # MIDDLE OF CART\n",
    "        # cart_location = get_cart_location(screen_width)\n",
    "        # if cart_location < view_width // 2:\n",
    "        #     slice_range = slice(view_width)\n",
    "        # elif cart_location > (screen_width - view_width // 2):\n",
    "        #     slice_range = slice(-view_width, None)\n",
    "        # else:\n",
    "        #     slice_range = slice(cart_location - view_width // 2,\n",
    "        #                         cart_location + view_width // 2)\n",
    "        # # Strip off the edges, so that we have a square image centered on a cart\n",
    "        # screen = screen[:, :, slice_range]\n",
    "        # Convert to float, rescale, convert to torch tensor\n",
    "        # (this doesn't require a copy)\n",
    "        screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "        screen = torch.from_numpy(screen)\n",
    "        # Resize, and add a batch dimension (BCHW)\n",
    "        resize = T.Compose([T.ToPILImage(),\n",
    "                        T.Resize(40, interpolation=InterpolationMode.BICUBIC),\n",
    "                        T.ToTensor()])\n",
    "        return resize(screen).unsqueeze(0)\n",
    "\n",
    "    _, c, h, w = get_screen(env).shape\n",
    "\n",
    "    # Create actor-critic module\n",
    "    ac = actor_critic(c, h, w, env.action_space, **ac_kwargs)\n",
    "\n",
    "    var_counts = {module[0]:core.count_vars(module[1]) for module in [['Pi', ac.pi], ['V', ac.v]]}\n",
    "    tb.add_scalars('Number of parameters', var_counts, 1)\n",
    "\n",
    "    # Set up experience buffer\n",
    "    local_steps_per_epoch = int(steps_per_epoch)\n",
    "    buf = VPGBuffer(obs_dim, c, h, w, act_dim, local_steps_per_epoch, gamma, lam)\n",
    "\n",
    "    # Set up function for computing VPG policy loss\n",
    "    def compute_loss_pi(data):\n",
    "        frames, obs, act, adv, logp_old = data['frames'], data['obs'], data['act'], data['adv'], data['logp']\n",
    "\n",
    "        # Policy loss\n",
    "        pi, logp = ac.pi(frames, act)\n",
    "        loss_pi = -(logp * adv).mean()\n",
    "\n",
    "        # Useful extra info\n",
    "        approx_kl = (logp_old - logp).mean().item()\n",
    "        ent = pi.entropy().mean().item()\n",
    "        pi_info = dict(kl=approx_kl, ent=ent)\n",
    "        \n",
    "        return loss_pi, pi_info\n",
    "\n",
    "    # Set up function for computing value loss\n",
    "    def compute_loss_v(data):\n",
    "        frames, obs, ret = data['frames'], data['obs'], data['ret']\n",
    "        return ((ac.v(frames) - ret)**2).mean()\n",
    "\n",
    "    # Set up optimizers for policy and value function\n",
    "    pi_optimizer = opt.Adam(ac.pi.parameters(), lr=pi_lr)\n",
    "    vf_optimizer = opt.Adam(ac.v.parameters(), lr=vf_lr)\n",
    "\n",
    "    def update(epoch):\n",
    "        data = buf.get()\n",
    "\n",
    "        # Get loss and info values before update\n",
    "        pi_l_old, pi_info_old = compute_loss_pi(data)\n",
    "        pi_l_old = pi_l_old.item()\n",
    "        v_l_old = compute_loss_v(data).item()\n",
    "\n",
    "        # Train policy with a single step of gradient descent\n",
    "        pi_optimizer.zero_grad()\n",
    "        loss_pi, pi_info = compute_loss_pi(data)\n",
    "        loss_pi.backward()\n",
    "        pi_optimizer.step()\n",
    "\n",
    "        # Value function learning\n",
    "        for i in range(train_v_iters):\n",
    "            vf_optimizer.zero_grad()\n",
    "            loss_v = compute_loss_v(data)\n",
    "            loss_v.backward()\n",
    "            vf_optimizer.step()\n",
    "\n",
    "        kl, ent = pi_info['kl'], pi_info_old['ent']\n",
    "        info = dict(LossPi=pi_l_old, LossV=v_l_old,\n",
    "                    KL=kl, Entropy=ent,\n",
    "                    DeltaLossPi=(loss_pi.item() - pi_l_old),\n",
    "                    DeltaLossV=(loss_v.item() - v_l_old))\n",
    "        tb.add_scalars('Info', info, epoch)\n",
    "\n",
    "    def generate_gif(ac, env, filename, T=200):\n",
    "        frames2 = []\n",
    "        s_t = env.reset()\n",
    "        reward2 = 0\n",
    "        for t in range(T):\n",
    "            # a_t = agent.act(s_t)\n",
    "            frame2 = get_screen(env)\n",
    "            # action_probs = ac.pi(torch.FloatTensor(frame)).detach().numpy().squeeze()\n",
    "            a2, v2, logp2 = ac.step(frame)\n",
    "            # action_probs = model(torch.FloatTensor(frame)).detach().numpy().squeeze()\n",
    "            # print(action_probs)\n",
    "            # action_probs = [0.5, 0.5]\n",
    "            # action = np.random.choice(env.action_space, p=action_probs)\n",
    "            s_2, r2, done2, info2 = env.step(a2[0])\n",
    "            # s_t, r_t, d_t, _ = env.step(action)\n",
    "            frame_2 = env.render(mode='rgb_array')\n",
    "            frames2.append(frame_2)\n",
    "            reward2 += r2\n",
    "            if done2:\n",
    "                break\n",
    "        print(reward2)\n",
    "        images_list = [Image.fromarray(frame2) for frame2 in frames2]\n",
    "        imageio.mimsave(f'{filename}.gif', frames2, duration=0.02)\n",
    "\n",
    "    # Prepare for interaction with environment\n",
    "    frame, o, ep_ret, ep_len = get_screen(env), env.reset(), 0, 0\n",
    "    ep_no = 0\n",
    "    # Main loop: collect experience in env and update/log each epoch\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        for t in tqdm(range(local_steps_per_epoch)):\n",
    "            a, v, logp = ac.step(torch.as_tensor(frame, dtype=torch.float32))\n",
    "\n",
    "            next_o, r, d, _ = env.step(a[0])\n",
    "            frame_ = get_screen(env)\n",
    "            ep_ret += r\n",
    "            ep_len += 1\n",
    "\n",
    "            # save and log\n",
    "            buf.store(frame, o, a, r, v, logp)\n",
    "            tb.add_scalar('VVals', v, epoch*local_steps_per_epoch+t)\n",
    "            # Update obs (critical!)\n",
    "            o = next_o\n",
    "            frame = frame_\n",
    "\n",
    "            timeout = ep_len == max_ep_len\n",
    "            terminal = d or timeout\n",
    "            epoch_ended = t==local_steps_per_epoch-1\n",
    "\n",
    "            if terminal or epoch_ended:\n",
    "                if epoch_ended and not(terminal):\n",
    "                    print('Warning: trajectory cut off by epoch at %d steps.'%ep_len, flush=True)\n",
    "                # if trajectory didn't reach terminal state, bootstrap value target\n",
    "                if timeout or epoch_ended:\n",
    "                    _, v, _ = ac.step(torch.as_tensor(frame, dtype=torch.float32))\n",
    "                else:\n",
    "                    v = 0\n",
    "                buf.finish_path(v)\n",
    "                if terminal:\n",
    "                    ep_no+=1\n",
    "                    # only save EpRet / EpLen if trajectory finished\n",
    "                    ep_data = dict(EpRet=ep_ret, EpLen=ep_len)\n",
    "                    tb.add_scalars('Episode data', ep_data, ep_no)\n",
    "                frame, o, ep_ret, ep_len = get_screen(env), env.reset(), 0, 0\n",
    "\n",
    "\n",
    "        # Save model\n",
    "        if (epoch % save_freq == 0) or (epoch == epochs-1):\n",
    "            torch.save(ac, './RL4_spinning_vpg5.pt')\n",
    "\n",
    "        # Perform VPG update!\n",
    "        update(epoch)\n",
    "        generate_gif(ac, env, './RL4_spinning_vpg_gifs2/%d.gif'%epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = \"Pendulum-v0\"\n",
    "# tb.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = SummaryWriter('Pendulum_0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Box' object has no attribute 'n'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6606/1595916934.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvpg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactor_critic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mActorCritic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_ep_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_6606/1556484630.py\u001b[0m in \u001b[0;36mvpg\u001b[0;34m(env_fn, actor_critic, ac_kwargs, seed, steps_per_epoch, epochs, gamma, pi_lr, vf_lr, train_v_iters, lam, max_ep_len, tb, save_freq)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;31m# Create actor-critic module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0mac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactor_critic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mac_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mvar_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount_vars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Pi'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mac\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'V'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mac\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CRISS/RL_algos/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, c, h, w, action_space)\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;31m# policy builder depends on action space\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGaussianActor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDiscrete\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCategoricalActor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Box' object has no attribute 'n'"
     ]
    }
   ],
   "source": [
    "vpg(lambda : gym.make(env), actor_critic=core.ActorCritic, tb=tb, seed = 1, steps_per_epoch=2000, max_ep_len=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_screen(env):                             \n",
    "    # Returned screen requested by gym is 400x600x3, but is sometimes larger\n",
    "    # such as 800x1200x3. Transpose it into torch order (CHW).\n",
    "    # time.sleep(2)\n",
    "    # env = env()\n",
    "    screen = env.render(mode='rgb_array')\n",
    "    screen  = screen.transpose((2, 0, 1))\n",
    "    # screen = env.render(mode='rgb_array').transpose((2, 0, 1))\n",
    "    # Cart is in the lower half, so strip off the top and bottom of the screen\n",
    "    _, screen_height, screen_width = screen.shape\n",
    "    screen = screen[:, int(screen_height*0.4):int(screen_height * 0.8)]\n",
    "    view_width = int(screen_width * 0.6)\n",
    "    def get_cart_location(screen_width):\n",
    "        world_width = env.x_threshold * 2\n",
    "        scale = screen_width / world_width\n",
    "        return int(env.state[0] * scale + screen_width / 2.0)  # MIDDLE OF CART\n",
    "    cart_location = get_cart_location(screen_width)\n",
    "    if cart_location < view_width // 2:\n",
    "        slice_range = slice(view_width)\n",
    "    elif cart_location > (screen_width - view_width // 2):\n",
    "        slice_range = slice(-view_width, None)\n",
    "    else:\n",
    "        slice_range = slice(cart_location - view_width // 2,\n",
    "                            cart_location + view_width // 2)\n",
    "    # Strip off the edges, so that we have a square image centered on a cart\n",
    "    screen = screen[:, :, slice_range]\n",
    "    # Convert to float, rescale, convert to torch tensor\n",
    "    # (this doesn't require a copy)\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    screen = torch.from_numpy(screen)\n",
    "    # Resize, and add a batch dimension (BCHW)\n",
    "    resize = T.Compose([T.ToPILImage(),\n",
    "                    T.Resize(40, interpolation=InterpolationMode.BICUBIC),\n",
    "                    T.ToTensor()])\n",
    "    return resize(screen).unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "env2 = gym.make(env)\n",
    "env2.reset()\n",
    "_, c, h, w = get_screen(env2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac = torch.load(\"./RL4_spinning_vpg2.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def generate_gif(env, filename, T=200):\n",
    "    frames = []\n",
    "    s_t = env.reset()\n",
    "    reward = 0\n",
    "    for t in range(T):\n",
    "        # a_t = agent.act(s_t)\n",
    "        frame = get_screen(env)\n",
    "        # action_probs = ac.pi(torch.FloatTensor(frame)).detach().numpy().squeeze()\n",
    "        a, v, logp = ac.step(frame)\n",
    "        # action_probs = model(torch.FloatTensor(frame)).detach().numpy().squeeze()\n",
    "        # print(action_probs)\n",
    "        # action_probs = [0.5, 0.5]\n",
    "        # action = np.random.choice(env.action_space, p=action_probs)\n",
    "        s_, r, done, info = env.step(a[0])\n",
    "        # s_t, r_t, d_t, _ = env.step(action)\n",
    "        frame_ = env.render(mode='rgb_array')\n",
    "        frames.append(frame_)\n",
    "        reward += r\n",
    "        if done:\n",
    "            break\n",
    "    print(reward)\n",
    "    images_list = [Image.fromarray(frame) for frame in frames]\n",
    "    imageio.mimsave(f'{filename}.gif', frames, duration=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.0\n"
     ]
    }
   ],
   "source": [
    "generate_gif(env2, \"demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d77854923f65db7c87e5b368fa3fd71bdf7a68487e1e3730c1972eb3f5d18515"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('RL')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
