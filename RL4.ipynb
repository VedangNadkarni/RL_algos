{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym.spaces import Discrete, Box\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as opt\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib\n",
    "import torchvision.transforms as T\n",
    "from torch.distributions import Categorical, Normal\n",
    "from scipy import signal\n",
    "import scipy\n",
    "from torch import optim\n",
    "import core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistics_scalar(x, with_min_and_max=False):\n",
    "    \"\"\"\n",
    "    Get mean/std and optional min/max of scalar x across MPI processes.\n",
    "\n",
    "    Args:\n",
    "        x: An array containing samples of the scalar to produce statistics\n",
    "            for.\n",
    "\n",
    "        with_min_and_max (bool): If true, return min and max of x in \n",
    "            addition to mean and std.\n",
    "    \"\"\"\n",
    "    x = np.array(x, dtype=np.float32)\n",
    "    global_sum, global_n = np.sum(x), len(x)\n",
    "    mean = global_sum / global_n\n",
    "\n",
    "    global_sum_sq = np.sum((x - mean)**2)\n",
    "    std = np.sqrt(global_sum_sq / global_n)  # compute global std\n",
    "\n",
    "    if with_min_and_max:\n",
    "        global_min =np.min(x) if len(x) > 0 else np.inf\n",
    "        global_max =np.max(x) if len(x) > 0 else -np.inf\n",
    "        return mean, std, global_min, global_max\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VPGBuffer:\n",
    "    def __init__(self, obs_dim, c, h, w, act_dim, size, gamma=0.99, lam=0.95):\n",
    "        self.obs_buf = np.zeros(core.combined_shape(size, obs_dim), dtype=np.float32)\n",
    "        self.frame_buf = np.zeros(core.combined_shape(size, c, h, w), dtype=np.float32)\n",
    "        self.obs_buf = np.zeros(core.combined_shape(size, obs_dim), dtype=np.float32)\n",
    "        self.act_buf = np.zeros(core.combined_shape(size, act_dim), dtype=np.float32)\n",
    "        self.adv_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.rew_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.ret_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.val_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.logp_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.gamma, self.lam = gamma, lam\n",
    "        self.ptr, self.path_start_idx, self.max_size = 0, 0, size\n",
    "\n",
    "    def store(self, frame, obs, act, rew, val, logp):\n",
    "        assert self.ptr < self.max_size     # buffer has to have room so you can store\n",
    "        self.obs_buf[self.ptr] = obs\n",
    "        self.frame_buf[self.ptr] = frame\n",
    "        self.act_buf[self.ptr] = act\n",
    "        self.rew_buf[self.ptr] = rew\n",
    "        self.val_buf[self.ptr] = val\n",
    "        self.logp_buf[self.ptr] = logp\n",
    "        self.ptr += 1\n",
    "\n",
    "    def finish_path(self, last_val=0):\n",
    "        path_slice = slice(self.path_start_idx, self.ptr)\n",
    "        rews = np.append(self.rew_buf[path_slice], last_val)\n",
    "        vals = np.append(self.val_buf[path_slice], last_val)\n",
    "        \n",
    "        # the next two lines implement GAE-Lambda advantage calculation\n",
    "        deltas = rews[:-1] + self.gamma * vals[1:] - vals[:-1]\n",
    "        self.adv_buf[path_slice] = core.discount_cumsum(deltas, self.gamma * self.lam)\n",
    "        \n",
    "        # the next line computes rewards-to-go, to be targets for the value function\n",
    "        self.ret_buf[path_slice] = core.discount_cumsum(rews, self.gamma)[:-1]\n",
    "        \n",
    "        self.path_start_idx = self.ptr\n",
    "\n",
    "    def get(self):\n",
    "        assert self.ptr == self.max_size    # buffer has to be full before you can get\n",
    "        self.ptr, self.path_start_idx = 0, 0\n",
    "        # the next two lines implement the advantage normalization trick\n",
    "        adv_mean, adv_std = statistics_scalar(self.adv_buf)\n",
    "        self.adv_buf = (self.adv_buf - adv_mean) / adv_std\n",
    "        data = dict(frames=self.frame_buf, obs=self.obs_buf, act=self.act_buf, ret=self.ret_buf,\n",
    "                    adv=self.adv_buf, logp=self.logp_buf)\n",
    "        return {k: torch.as_tensor(v, dtype=torch.float32) for k,v in data.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import InterpolationMode\n",
    "resize = T.Compose([T.ToPILImage(),\n",
    "                    T.Resize(40, interpolation=InterpolationMode.BICUBIC),\n",
    "                    T.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vpg(env, actor_critic=core.ActorCritic, ac_kwargs=dict(),  seed=0, \n",
    "        steps_per_epoch=4000, epochs=50, gamma=0.99, pi_lr=3e-4,\n",
    "        vf_lr=1e-3, train_v_iters=80, lam=0.97, max_ep_len=1000,\n",
    "        logger_kwargs=dict(), save_freq=10):\n",
    "    \n",
    "    # Random seed\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Instantiate environment\n",
    "    # env = env_fn()\n",
    "    obs_dim = env.observation_space.shape\n",
    "    act_dim = env.action_space.shape\n",
    "    env.render()\n",
    "    \n",
    "    def get_screen():                             \n",
    "        # Returned screen requested by gym is 400x600x3, but is sometimes larger\n",
    "        # such as 800x1200x3. Transpose it into torch order (CHW).\n",
    "        # time.sleep(2)\n",
    "        # env = env()\n",
    "        screen = env.render(mode='rgb_array')\n",
    "        print(screen)\n",
    "        screen  = screen.transpose((2, 0, 1))\n",
    "        # screen = env.render(mode='rgb_array').transpose((2, 0, 1))\n",
    "        # Cart is in the lower half, so strip off the top and bottom of the screen\n",
    "        _, screen_height, screen_width = screen.shape\n",
    "        screen = screen[:, int(screen_height*0.4):int(screen_height * 0.8)]\n",
    "        view_width = int(screen_width * 0.6)\n",
    "        def get_cart_location(screen_width):\n",
    "            world_width = env.x_threshold * 2\n",
    "            scale = screen_width / world_width\n",
    "            return int(env.state[0] * scale + screen_width / 2.0)  # MIDDLE OF CART\n",
    "        cart_location = get_cart_location(screen_width)\n",
    "        if cart_location < view_width // 2:\n",
    "            slice_range = slice(view_width)\n",
    "        elif cart_location > (screen_width - view_width // 2):\n",
    "            slice_range = slice(-view_width, None)\n",
    "        else:\n",
    "            slice_range = slice(cart_location - view_width // 2,\n",
    "                                cart_location + view_width // 2)\n",
    "        # Strip off the edges, so that we have a square image centered on a cart\n",
    "        screen = screen[:, :, slice_range]\n",
    "        # Convert to float, rescale, convert to torch tensor\n",
    "        # (this doesn't require a copy)\n",
    "        screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "        screen = torch.from_numpy(screen)\n",
    "        # Resize, and add a batch dimension (BCHW)\n",
    "        resize = T.Compose([T.ToPILImage(),\n",
    "                        T.Resize(40, interpolation=InterpolationMode.BICUBIC),\n",
    "                        T.ToTensor()])\n",
    "        return resize(screen).unsqueeze(0)\n",
    "\n",
    "    _, c, h, w = get_screen().shape\n",
    "\n",
    "    # Create actor-critic module\n",
    "    ac = actor_critic(c, h, w, env.action_space, **ac_kwargs)\n",
    "\n",
    "    # Set up experience buffer\n",
    "    local_steps_per_epoch = int(steps_per_epoch)\n",
    "    buf = VPGBuffer(obs_dim, act_dim, local_steps_per_epoch, gamma, lam)\n",
    "\n",
    "    # Set up function for computing VPG policy loss\n",
    "    def compute_loss_pi(data):\n",
    "        frames, obs, act, adv, logp_old = data['frames'], data['obs'], data['act'], data['adv'], data['logp']\n",
    "\n",
    "        # Policy loss\n",
    "        pi, logp = ac.pi(c, h, w, act)\n",
    "        loss_pi = -(logp * adv).mean()\n",
    "\n",
    "        # Useful extra info\n",
    "        approx_kl = (logp_old - logp).mean().item()\n",
    "        ent = pi.entropy().mean().item()\n",
    "        pi_info = dict(kl=approx_kl, ent=ent)\n",
    "        \n",
    "        return loss_pi, pi_info\n",
    "\n",
    "    # Set up function for computing value loss\n",
    "    def compute_loss_v(data):\n",
    "        obs, ret = data['obs'], data['ret']\n",
    "        return ((ac.v(obs) - ret)**2).mean()\n",
    "\n",
    "    # Set up optimizers for policy and value function\n",
    "    pi_optimizer = opt.Adam(ac.pi.parameters(), lr=pi_lr)\n",
    "    vf_optimizer = opt.Adam(ac.v.parameters(), lr=vf_lr)\n",
    "\n",
    "    def update():\n",
    "        data = buf.get()\n",
    "\n",
    "        # Get loss and info values before update\n",
    "        pi_l_old, pi_info_old = compute_loss_pi(data)\n",
    "        pi_l_old = pi_l_old.item()\n",
    "        v_l_old = compute_loss_v(data).item()\n",
    "\n",
    "        # Train policy with a single step of gradient descent\n",
    "        pi_optimizer.zero_grad()\n",
    "        loss_pi, pi_info = compute_loss_pi(data)\n",
    "        loss_pi.backward()\n",
    "        pi_optimizer.step()\n",
    "\n",
    "        # Value function learning\n",
    "        for i in range(train_v_iters):\n",
    "            vf_optimizer.zero_grad()\n",
    "            loss_v = compute_loss_v(data)\n",
    "            loss_v.backward()\n",
    "            vf_optimizer.step()\n",
    "\n",
    "    # Prepare for interaction with environment\n",
    "    frame, o, ep_ret, ep_len = get_screen(), env.reset(), 0, 0\n",
    "\n",
    "    # Main loop: collect experience in env and update/log each epoch\n",
    "    for epoch in range(epochs):\n",
    "        for t in range(local_steps_per_epoch):\n",
    "            a, v, logp = ac.step(torch.as_tensor(o, dtype=torch.float32))\n",
    "\n",
    "            next_o, r, d, _ = env.step(a)\n",
    "            frame_ = get_screen()\n",
    "            ep_ret += r\n",
    "            ep_len += 1\n",
    "\n",
    "            # save and log\n",
    "            buf.store(frame, o, a, r, v, logp)\n",
    "            \n",
    "            # Update obs (critical!)\n",
    "            o = next_o\n",
    "            frame = frame_\n",
    "\n",
    "            timeout = ep_len == max_ep_len\n",
    "            terminal = d or timeout\n",
    "            epoch_ended = t==local_steps_per_epoch-1\n",
    "\n",
    "            if terminal or epoch_ended:\n",
    "                if epoch_ended and not(terminal):\n",
    "                    print('Warning: trajectory cut off by epoch at %d steps.'%ep_len, flush=True)\n",
    "                # if trajectory didn't reach terminal state, bootstrap value target\n",
    "                if timeout or epoch_ended:\n",
    "                    _, v, _ = ac.step(torch.as_tensor(o, dtype=torch.float32))\n",
    "                else:\n",
    "                    v = 0\n",
    "                buf.finish_path(v)\n",
    "                frame, o, ep_ret, ep_len = get_screen(), env.reset(), 0, 0\n",
    "\n",
    "\n",
    "        # Save model\n",
    "        if (epoch % save_freq == 0) or (epoch == epochs-1):\n",
    "            torch.save(ac, f'./RL4_spinning_vpg')\n",
    "\n",
    "        # Perform VPG update!\n",
    "        update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = \"CartPole-v0\"\n",
    "env = gym.make(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'transpose'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_15073/2374901507.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvpg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactor_critic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mActorCritic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_15073/2190341236.py\u001b[0m in \u001b[0;36mvpg\u001b[0;34m(env, actor_critic, ac_kwargs, seed, steps_per_epoch, epochs, gamma, pi_lr, vf_lr, train_v_iters, lam, max_ep_len, logger_kwargs, save_freq)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_screen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;31m# Create actor-critic module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_15073/2190341236.py\u001b[0m in \u001b[0;36mget_screen\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mscreen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rgb_array'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mscreen\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;31m# screen = env.render(mode='rgb_array').transpose((2, 0, 1))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# Cart is in the lower half, so strip off the top and bottom of the screen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'transpose'"
     ]
    }
   ],
   "source": [
    "vpg(env, actor_critic=core.ActorCritic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'transpose'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_15073/3991975717.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_screen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;31m# Create actor-critic module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_15073/3991975717.py\u001b[0m in \u001b[0;36mget_screen\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rgb_array'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mscreen\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0;31m# screen = env.render(mode='rgb_array').transpose((2, 0, 1))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m# Cart is in the lower half, so strip off the top and bottom of the screen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'transpose'"
     ]
    }
   ],
   "source": [
    "env\n",
    "actor_critic=core.ActorCritic\n",
    "ac_kwargs=dict()\n",
    "seed=0 \n",
    "steps_per_epoch=4000\n",
    "epochs=50\n",
    "gamma=0.99\n",
    "pi_lr=3e-4\n",
    "vf_lr=1e-3\n",
    "train_v_iters=80\n",
    "lam=0.97\n",
    "max_ep_len=1000\n",
    "logger_kwargs=dict()\n",
    "save_freq=10\n",
    "# Random seed\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Instantiate environment\n",
    "# env = env_fn()\n",
    "obs_dim = env.observation_space.shape\n",
    "act_dim = env.action_space.shape\n",
    "env.render()\n",
    "\n",
    "def get_screen():                             \n",
    "    # Returned screen requested by gym is 400x600x3, but is sometimes larger\n",
    "    # such as 800x1200x3. Transpose it into torch order (CHW).\n",
    "    # time.sleep(2)\n",
    "    # env = env()\n",
    "    screen = env.render(mode='rgb_array')\n",
    "    print(screen)\n",
    "    print(env.render('rgb_array'))\n",
    "    screen  = screen.transpose((2, 0, 1))\n",
    "    # screen = env.render(mode='rgb_array').transpose((2, 0, 1))\n",
    "    # Cart is in the lower half, so strip off the top and bottom of the screen\n",
    "    _, screen_height, screen_width = screen.shape\n",
    "    screen = screen[:, int(screen_height*0.4):int(screen_height * 0.8)]\n",
    "    view_width = int(screen_width * 0.6)\n",
    "    def get_cart_location(screen_width):\n",
    "        world_width = env.x_threshold * 2\n",
    "        scale = screen_width / world_width\n",
    "        return int(env.state[0] * scale + screen_width / 2.0)  # MIDDLE OF CART\n",
    "    cart_location = get_cart_location(screen_width)\n",
    "    if cart_location < view_width // 2:\n",
    "        slice_range = slice(view_width)\n",
    "    elif cart_location > (screen_width - view_width // 2):\n",
    "        slice_range = slice(-view_width, None)\n",
    "    else:\n",
    "        slice_range = slice(cart_location - view_width // 2,\n",
    "                            cart_location + view_width // 2)\n",
    "    # Strip off the edges, so that we have a square image centered on a cart\n",
    "    screen = screen[:, :, slice_range]\n",
    "    # Convert to float, rescale, convert to torch tensor\n",
    "    # (this doesn't require a copy)\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    screen = torch.from_numpy(screen)\n",
    "    # Resize, and add a batch dimension (BCHW)\n",
    "    resize = T.Compose([T.ToPILImage(),\n",
    "                    T.Resize(40, interpolation=InterpolationMode.BICUBIC),\n",
    "                    T.ToTensor()])\n",
    "    return resize(screen).unsqueeze(0)\n",
    "\n",
    "_, c, h, w = get_screen().shape\n",
    "\n",
    "# Create actor-critic module\n",
    "ac = actor_critic(c, h, w, env.action_space, **ac_kwargs)\n",
    "\n",
    "# Set up experience buffer\n",
    "local_steps_per_epoch = int(steps_per_epoch)\n",
    "buf = VPGBuffer(obs_dim, act_dim, local_steps_per_epoch, gamma, lam)\n",
    "\n",
    "# Set up function for computing VPG policy loss\n",
    "def compute_loss_pi(data):\n",
    "    frames, obs, act, adv, logp_old = data['frames'], data['obs'], data['act'], data['adv'], data['logp']\n",
    "\n",
    "    # Policy loss\n",
    "    pi, logp = ac.pi(c, h, w, act)\n",
    "    loss_pi = -(logp * adv).mean()\n",
    "\n",
    "    # Useful extra info\n",
    "    approx_kl = (logp_old - logp).mean().item()\n",
    "    ent = pi.entropy().mean().item()\n",
    "    pi_info = dict(kl=approx_kl, ent=ent)\n",
    "    \n",
    "    return loss_pi, pi_info\n",
    "\n",
    "# Set up function for computing value loss\n",
    "def compute_loss_v(data):\n",
    "    obs, ret = data['obs'], data['ret']\n",
    "    return ((ac.v(obs) - ret)**2).mean()\n",
    "\n",
    "# Set up optimizers for policy and value function\n",
    "pi_optimizer = opt.Adam(ac.pi.parameters(), lr=pi_lr)\n",
    "vf_optimizer = opt.Adam(ac.v.parameters(), lr=vf_lr)\n",
    "\n",
    "def update():\n",
    "    data = buf.get()\n",
    "\n",
    "    # Get loss and info values before update\n",
    "    pi_l_old, pi_info_old = compute_loss_pi(data)\n",
    "    pi_l_old = pi_l_old.item()\n",
    "    v_l_old = compute_loss_v(data).item()\n",
    "\n",
    "    # Train policy with a single step of gradient descent\n",
    "    pi_optimizer.zero_grad()\n",
    "    loss_pi, pi_info = compute_loss_pi(data)\n",
    "    loss_pi.backward()\n",
    "    pi_optimizer.step()\n",
    "\n",
    "    # Value function learning\n",
    "    for i in range(train_v_iters):\n",
    "        vf_optimizer.zero_grad()\n",
    "        loss_v = compute_loss_v(data)\n",
    "        loss_v.backward()\n",
    "        vf_optimizer.step()\n",
    "\n",
    "# Prepare for interaction with environment\n",
    "frame, o, ep_ret, ep_len = get_screen(), env.reset(), 0, 0\n",
    "\n",
    "# Main loop: collect experience in env and update/log each epoch\n",
    "for epoch in range(epochs):\n",
    "    for t in range(local_steps_per_epoch):\n",
    "        a, v, logp = ac.step(torch.as_tensor(o, dtype=torch.float32))\n",
    "\n",
    "        next_o, r, d, _ = env.step(a)\n",
    "        frame_ = get_screen()\n",
    "        ep_ret += r\n",
    "        ep_len += 1\n",
    "\n",
    "        # save and log\n",
    "        buf.store(frame, o, a, r, v, logp)\n",
    "        \n",
    "        # Update obs (critical!)\n",
    "        o = next_o\n",
    "        frame = frame_\n",
    "\n",
    "        timeout = ep_len == max_ep_len\n",
    "        terminal = d or timeout\n",
    "        epoch_ended = t==local_steps_per_epoch-1\n",
    "\n",
    "        if terminal or epoch_ended:\n",
    "            if epoch_ended and not(terminal):\n",
    "                print('Warning: trajectory cut off by epoch at %d steps.'%ep_len, flush=True)\n",
    "            # if trajectory didn't reach terminal state, bootstrap value target\n",
    "            if timeout or epoch_ended:\n",
    "                _, v, _ = ac.step(torch.as_tensor(o, dtype=torch.float32))\n",
    "            else:\n",
    "                v = 0\n",
    "            buf.finish_path(v)\n",
    "            frame, o, ep_ret, ep_len = get_screen(), env.reset(), 0, 0\n",
    "\n",
    "\n",
    "    # Save model\n",
    "    if (epoch % save_freq == 0) or (epoch == epochs-1):\n",
    "        torch.save(ac, f'./RL4_spinning_vpg')\n",
    "\n",
    "    # Perform VPG update!\n",
    "    update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d77854923f65db7c87e5b368fa3fd71bdf7a68487e1e3730c1972eb3f5d18515"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('RL')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
