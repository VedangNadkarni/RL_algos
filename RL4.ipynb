{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym.spaces import Discrete, Box\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as opt\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib\n",
    "import torchvision.transforms as T\n",
    "from torch.distributions import Categorical, Normal\n",
    "from scipy import signal\n",
    "import scipy\n",
    "from torch import optim\n",
    "import core\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from torch.utils.tensorboard import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistics_scalar(x, with_min_and_max=False):\n",
    "    \"\"\"\n",
    "    Get mean/std and optional min/max of scalar x across MPI processes.\n",
    "\n",
    "    Args:\n",
    "        x: An array containing samples of the scalar to produce statistics\n",
    "            for.\n",
    "\n",
    "        with_min_and_max (bool): If true, return min and max of x in \n",
    "            addition to mean and std.\n",
    "    \"\"\"\n",
    "    x = np.array(x, dtype=np.float32)\n",
    "    global_sum, global_n = np.sum(x), len(x)\n",
    "    mean = global_sum / global_n\n",
    "\n",
    "    global_sum_sq = np.sum((x - mean)**2)\n",
    "    std = np.sqrt(global_sum_sq / global_n)  # compute global std\n",
    "\n",
    "    if with_min_and_max:\n",
    "        global_min =np.min(x) if len(x) > 0 else np.inf\n",
    "        global_max =np.max(x) if len(x) > 0 else -np.inf\n",
    "        return mean, std, global_min, global_max\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VPGBuffer:\n",
    "    def __init__(self, obs_dim, c, h, w, act_dim, size, gamma=0.99, lam=0.95):\n",
    "        self.obs_buf = np.zeros(core.combined_shape(size, obs_dim), dtype=np.float32)\n",
    "        self.frame_buf = np.zeros(core.combined_shape(size, (c, h, w)), dtype=np.float32)\n",
    "        self.obs_buf = np.zeros(core.combined_shape(size, obs_dim), dtype=np.float32)\n",
    "        self.act_buf = np.zeros(core.combined_shape(size, act_dim), dtype=np.float32)\n",
    "        self.adv_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.rew_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.ret_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.val_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.logp_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.gamma, self.lam = gamma, lam\n",
    "        self.ptr, self.path_start_idx, self.max_size = 0, 0, size\n",
    "\n",
    "    def store(self, frame, obs, act, rew, val, logp):\n",
    "        assert self.ptr < self.max_size     # buffer has to have room so you can store\n",
    "        self.obs_buf[self.ptr] = obs\n",
    "        self.frame_buf[self.ptr] = frame\n",
    "        self.act_buf[self.ptr] = act\n",
    "        self.rew_buf[self.ptr] = rew\n",
    "        self.val_buf[self.ptr] = val\n",
    "        self.logp_buf[self.ptr] = logp\n",
    "        self.ptr += 1\n",
    "\n",
    "    def finish_path(self, last_val=0):\n",
    "        path_slice = slice(self.path_start_idx, self.ptr)\n",
    "        rews = np.append(self.rew_buf[path_slice], last_val)\n",
    "        vals = np.append(self.val_buf[path_slice], last_val)\n",
    "        \n",
    "        # the next two lines implement GAE-Lambda advantage calculation\n",
    "        deltas = rews[:-1] + self.gamma * vals[1:] - vals[:-1]\n",
    "        self.adv_buf[path_slice] = core.discount_cumsum(deltas, self.gamma * self.lam)\n",
    "        \n",
    "        # the next line computes rewards-to-go, to be targets for the value function\n",
    "        self.ret_buf[path_slice] = core.discount_cumsum(rews, self.gamma)[:-1]\n",
    "        \n",
    "        self.path_start_idx = self.ptr\n",
    "\n",
    "    def get(self):\n",
    "        assert self.ptr == self.max_size    # buffer has to be full before you can get\n",
    "        self.ptr, self.path_start_idx = 0, 0\n",
    "        # the next two lines implement the advantage normalization trick\n",
    "        adv_mean, adv_std = statistics_scalar(self.adv_buf)\n",
    "        self.adv_buf = (self.adv_buf - adv_mean) / adv_std\n",
    "        data = dict(frames=self.frame_buf, obs=self.obs_buf, act=self.act_buf, ret=self.ret_buf,\n",
    "                    adv=self.adv_buf, logp=self.logp_buf)\n",
    "        return {k: torch.as_tensor(v, dtype=torch.float32) for k,v in data.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = 'CartPole-v0'\n",
    "# env = gym.make('CartPole-v0')\n",
    "def vpg(env_fn, actor_critic=core.ActorCritic, ac_kwargs=dict(),  seed=0, \n",
    "        steps_per_epoch=40, epochs=50, gamma=0.99, pi_lr=3e-4,\n",
    "        vf_lr=1e-3, train_v_iters=80, lam=0.97, max_ep_len=1000,\n",
    "        tb=SummaryWriter(), save_freq=10):\n",
    "    \n",
    "    # Random seed\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Instantiate environment\n",
    "    env = env_fn()\n",
    "    obs_dim = env.observation_space.shape\n",
    "    act_dim = env.action_space.shape\n",
    "    env.reset()\n",
    "    env.render()\n",
    "    \n",
    "    def get_screen(env):                             \n",
    "        # Returned screen requested by gym is 400x600x3, but is sometimes larger\n",
    "        # such as 800x1200x3. Transpose it into torch order (CHW).\n",
    "        # time.sleep(2)\n",
    "        # env = env()\n",
    "        screen = env.render(mode='rgb_array')\n",
    "        screen  = screen.transpose((2, 0, 1))\n",
    "        # screen = env.render(mode='rgb_array').transpose((2, 0, 1))\n",
    "        # Cart is in the lower half, so strip off the top and bottom of the screen\n",
    "        _, screen_height, screen_width = screen.shape\n",
    "        screen = screen[:, int(screen_height*0.4):int(screen_height * 0.8)]\n",
    "        view_width = int(screen_width * 0.6)\n",
    "        def get_cart_location(screen_width):\n",
    "            world_width = env.x_threshold * 2\n",
    "            scale = screen_width / world_width\n",
    "            return int(env.state[0] * scale + screen_width / 2.0)  # MIDDLE OF CART\n",
    "        cart_location = get_cart_location(screen_width)\n",
    "        if cart_location < view_width // 2:\n",
    "            slice_range = slice(view_width)\n",
    "        elif cart_location > (screen_width - view_width // 2):\n",
    "            slice_range = slice(-view_width, None)\n",
    "        else:\n",
    "            slice_range = slice(cart_location - view_width // 2,\n",
    "                                cart_location + view_width // 2)\n",
    "        # Strip off the edges, so that we have a square image centered on a cart\n",
    "        screen = screen[:, :, slice_range]\n",
    "        # Convert to float, rescale, convert to torch tensor\n",
    "        # (this doesn't require a copy)\n",
    "        screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "        screen = torch.from_numpy(screen)\n",
    "        # Resize, and add a batch dimension (BCHW)\n",
    "        resize = T.Compose([T.ToPILImage(),\n",
    "                        T.Resize(40, interpolation=InterpolationMode.BICUBIC),\n",
    "                        T.ToTensor()])\n",
    "        return resize(screen).unsqueeze(0)\n",
    "\n",
    "    _, c, h, w = get_screen(env).shape\n",
    "\n",
    "    # Create actor-critic module\n",
    "    ac = actor_critic(c, h, w, env.action_space, **ac_kwargs)\n",
    "\n",
    "    var_counts = {module:core.count_vars(module) for module in [ac.pi, ac.v]}\n",
    "    tb.add_scalars('Number of parameters', var_counts, 1)\n",
    "\n",
    "    # Set up experience buffer\n",
    "    local_steps_per_epoch = int(steps_per_epoch)\n",
    "    buf = VPGBuffer(obs_dim, c, h, w, act_dim, local_steps_per_epoch, gamma, lam)\n",
    "\n",
    "    # Set up function for computing VPG policy loss\n",
    "    def compute_loss_pi(data):\n",
    "        frames, obs, act, adv, logp_old = data['frames'], data['obs'], data['act'], data['adv'], data['logp']\n",
    "\n",
    "        # Policy loss\n",
    "        pi, logp = ac.pi(frames, act)\n",
    "        loss_pi = -(logp * adv).mean()\n",
    "\n",
    "        # Useful extra info\n",
    "        approx_kl = (logp_old - logp).mean().item()\n",
    "        ent = pi.entropy().mean().item()\n",
    "        pi_info = dict(kl=approx_kl, ent=ent)\n",
    "        \n",
    "        return loss_pi, pi_info\n",
    "\n",
    "    # Set up function for computing value loss\n",
    "    def compute_loss_v(data):\n",
    "        frames, obs, ret = data['frames'], data['obs'], data['ret']\n",
    "        return ((ac.v(frames) - ret)**2).mean()\n",
    "\n",
    "    # Set up optimizers for policy and value function\n",
    "    pi_optimizer = opt.Adam(ac.pi.parameters(), lr=pi_lr)\n",
    "    vf_optimizer = opt.Adam(ac.v.parameters(), lr=vf_lr)\n",
    "\n",
    "    def update(epoch):\n",
    "        data = buf.get()\n",
    "\n",
    "        # Get loss and info values before update\n",
    "        pi_l_old, pi_info_old = compute_loss_pi(data)\n",
    "        pi_l_old = pi_l_old.item()\n",
    "        v_l_old = compute_loss_v(data).item()\n",
    "\n",
    "        # Train policy with a single step of gradient descent\n",
    "        pi_optimizer.zero_grad()\n",
    "        loss_pi, pi_info = compute_loss_pi(data)\n",
    "        loss_pi.backward()\n",
    "        pi_optimizer.step()\n",
    "\n",
    "        # Value function learning\n",
    "        for i in range(train_v_iters):\n",
    "            vf_optimizer.zero_grad()\n",
    "            loss_v = compute_loss_v(data)\n",
    "            loss_v.backward()\n",
    "            vf_optimizer.step()\n",
    "\n",
    "        kl, ent = pi_info['kl'], pi_info_old['ent']\n",
    "        info = dict(LossPi=pi_l_old, LossV=v_l_old,\n",
    "                    KL=kl, Entropy=ent,\n",
    "                    DeltaLossPi=(loss_pi.item() - pi_l_old),\n",
    "                    DeltaLossV=(loss_v.item() - v_l_old))\n",
    "        tb.add_scalars('Info', info, epoch)\n",
    "\n",
    "    # Prepare for interaction with environment\n",
    "    frame, o, ep_ret, ep_len = get_screen(env), env.reset(), 0, 0\n",
    "    ep_no = 0\n",
    "    # Main loop: collect experience in env and update/log each epoch\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        for t in tqdm(range(local_steps_per_epoch)):\n",
    "            a, v, logp = ac.step(torch.as_tensor(frame, dtype=torch.float32))\n",
    "\n",
    "            next_o, r, d, _ = env.step(a[0])\n",
    "            frame_ = get_screen(env)\n",
    "            ep_ret += r\n",
    "            ep_len += 1\n",
    "\n",
    "            # save and log\n",
    "            buf.store(frame, o, a, r, v, logp)\n",
    "            tb.add_scalar('VVals', v, epoch*local_steps_per_epoch+t)\n",
    "            # Update obs (critical!)\n",
    "            o = next_o\n",
    "            frame = frame_\n",
    "\n",
    "            timeout = ep_len == max_ep_len\n",
    "            terminal = d or timeout\n",
    "            epoch_ended = t==local_steps_per_epoch-1\n",
    "\n",
    "            if terminal or epoch_ended:\n",
    "                if epoch_ended and not(terminal):\n",
    "                    print('Warning: trajectory cut off by epoch at %d steps.'%ep_len, flush=True)\n",
    "                # if trajectory didn't reach terminal state, bootstrap value target\n",
    "                if timeout or epoch_ended:\n",
    "                    _, v, _ = ac.step(torch.as_tensor(frame, dtype=torch.float32))\n",
    "                else:\n",
    "                    v = 0\n",
    "                buf.finish_path(v)\n",
    "                if terminal:\n",
    "                    ep_no+=1\n",
    "                    # only save EpRet / EpLen if trajectory finished\n",
    "                    ep_data = dict(EpRet=ep_ret, EpLen=ep_len)\n",
    "                    tb.add_scalars('Episode data', ep_data, ep_no)\n",
    "                frame, o, ep_ret, ep_len = get_screen(env), env.reset(), 0, 0\n",
    "\n",
    "\n",
    "        # Save model\n",
    "        if (epoch % save_freq == 0) or (epoch == epochs-1):\n",
    "            torch.save(ac, f'./RL4_spinning_vpg')\n",
    "\n",
    "        # Perform VPG update!\n",
    "        update(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "XIO:  fatal IO error 0 (Success) on X server \":0\"\n",
      "      after 1741 requests (1741 known processed) with 14 events remaining.\n"
     ]
    }
   ],
   "source": [
    "env = \"CartPole-v0\"\n",
    "vpg(lambda : gym.make(env), actor_critic=core.ActorCritic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "actor_critic=core.ActorCritic\n",
    "ac_kwargs=dict()\n",
    "seed=0 \n",
    "steps_per_epoch=4000\n",
    "epochs=50\n",
    "gamma=0.99\n",
    "pi_lr=3e-4\n",
    "vf_lr=1e-3\n",
    "train_v_iters=80\n",
    "lam=0.97\n",
    "max_ep_len=1000\n",
    "logger_kwargs=dict()\n",
    "save_freq=10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random seed\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Instantiate environment\n",
    "# env = env_fn()\n",
    "env.reset()\n",
    "obs_dim = env.observation_space.shape\n",
    "act_dim = env.action_space.shape\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_screen():                             \n",
    "    # Returned screen requested by gym is 400x600x3, but is sometimes larger\n",
    "    # such as 800x1200x3. Transpose it into torch order (CHW).\n",
    "    # time.sleep(2)\n",
    "    # env = env()\n",
    "    screen = env.render(mode='rgb_array')\n",
    "    # print(screen)\n",
    "    # print(env.render('rgb_array'))\n",
    "    screen  = screen.transpose((2, 0, 1))\n",
    "    # screen = env.render(mode='rgb_array').transpose((2, 0, 1))\n",
    "    # Cart is in the lower half, so strip off the top and bottom of the screen\n",
    "    _, screen_height, screen_width = screen.shape\n",
    "    screen = screen[:, int(screen_height*0.4):int(screen_height * 0.8)]\n",
    "    view_width = int(screen_width * 0.6)\n",
    "    def get_cart_location(screen_width):\n",
    "        world_width = env.x_threshold * 2\n",
    "        scale = screen_width / world_width\n",
    "        return int(env.state[0] * scale + screen_width / 2.0)  # MIDDLE OF CART\n",
    "    cart_location = get_cart_location(screen_width)\n",
    "    if cart_location < view_width // 2:\n",
    "        slice_range = slice(view_width)\n",
    "    elif cart_location > (screen_width - view_width // 2):\n",
    "        slice_range = slice(-view_width, None)\n",
    "    else:\n",
    "        slice_range = slice(cart_location - view_width // 2,\n",
    "                            cart_location + view_width // 2)\n",
    "    # Strip off the edges, so that we have a square image centered on a cart\n",
    "    screen = screen[:, :, slice_range]\n",
    "    # Convert to float, rescale, convert to torch tensor\n",
    "    # (this doesn't require a copy)\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    screen = torch.from_numpy(screen)\n",
    "    # Resize, and add a batch dimension (BCHW)\n",
    "    resize = T.Compose([T.ToPILImage(),\n",
    "                    T.Resize(40, interpolation=InterpolationMode.BICUBIC),\n",
    "                    T.ToTensor()])\n",
    "    return resize(screen).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, c, h, w = get_screen().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create actor-critic module\n",
    "ac = actor_critic(c, h, w, env.action_space, **ac_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_counts = tuple(core.count_vars(module) for module in [ac.pi, ac.v])\n",
    "tb = SummaryWriter()\n",
    "tb.close()\n",
    "tb.add_scalar('\\nNumber of parameters: \\t pi: %d, \\t v: %d\\n'%var_counts)\n",
    "tb.add\n",
    "\n",
    "# Set up experience buffer\n",
    "local_steps_per_epoch = int(steps_per_epoch)\n",
    "buf = VPGBuffer(obs_dim, c, h, w, act_dim, local_steps_per_epoch, gamma, lam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'local_steps_per_epoch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2879/3886075573.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;31m# Main loop: collect experience in env and update/log each epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_steps_per_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mac\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'local_steps_per_epoch' is not defined"
     ]
    }
   ],
   "source": [
    "# Set up function for computing VPG policy loss\n",
    "def compute_loss_pi(data):\n",
    "    frames, obs, act, adv, logp_old = data['frames'], data['obs'], data['act'], data['adv'], data['logp']\n",
    "\n",
    "    # Policy loss\n",
    "    pi, logp = ac.pi(frames, act)\n",
    "    loss_pi = -(logp * adv).mean()\n",
    "\n",
    "    # Useful extra info\n",
    "    approx_kl = (logp_old - logp).mean().item()\n",
    "    ent = pi.entropy().mean().item()\n",
    "    pi_info = dict(kl=approx_kl, ent=ent)\n",
    "    \n",
    "    return loss_pi, pi_info\n",
    "\n",
    "# Set up function for computing value loss\n",
    "def compute_loss_v(data):\n",
    "    obs, ret = data['obs'], data['ret']\n",
    "    return ((ac.v(obs) - ret)**2).mean()\n",
    "\n",
    "# Set up optimizers for policy and value function\n",
    "pi_optimizer = opt.Adam(ac.pi.parameters(), lr=pi_lr)\n",
    "vf_optimizer = opt.Adam(ac.v.parameters(), lr=vf_lr)\n",
    "\n",
    "def update():\n",
    "    data = buf.get()\n",
    "\n",
    "    # Get loss and info values before update\n",
    "    pi_l_old, pi_info_old = compute_loss_pi(data)\n",
    "    pi_l_old = pi_l_old.item()\n",
    "    v_l_old = compute_loss_v(data).item()\n",
    "\n",
    "    # Train policy with a single step of gradient descent\n",
    "    pi_optimizer.zero_grad()\n",
    "    loss_pi, pi_info = compute_loss_pi(data)\n",
    "    loss_pi.backward()\n",
    "    pi_optimizer.step()\n",
    "\n",
    "    # Value function learning\n",
    "    for i in range(train_v_iters):\n",
    "        vf_optimizer.zero_grad()\n",
    "        loss_v = compute_loss_v(data)\n",
    "        loss_v.backward()\n",
    "        vf_optimizer.step()\n",
    "\n",
    "# Prepare for interaction with environment\n",
    "frame, o, ep_ret, ep_len = get_screen(), env.reset(), 0, 0\n",
    "\n",
    "# Main loop: collect experience in env and update/log each epoch\n",
    "for epoch in range(epochs):\n",
    "    for t in range(local_steps_per_epoch):\n",
    "        a, v, logp = ac.step(torch.as_tensor(o, dtype=torch.float32))\n",
    "\n",
    "        next_o, r, d, _ = env.step(a)\n",
    "        frame_ = get_screen()\n",
    "        ep_ret += r\n",
    "        ep_len += 1\n",
    "\n",
    "        # save and log\n",
    "        buf.store(frame, o, a, r, v, logp)\n",
    "        \n",
    "        # Update obs (critical!)\n",
    "        o = next_o\n",
    "        frame = frame_\n",
    "\n",
    "        timeout = ep_len == max_ep_len\n",
    "        terminal = d or timeout\n",
    "        epoch_ended = t==local_steps_per_epoch-1\n",
    "\n",
    "        if terminal or epoch_ended:\n",
    "            if epoch_ended and not(terminal):\n",
    "                print('Warning: trajectory cut off by epoch at %d steps.'%ep_len, flush=True)\n",
    "            # if trajectory didn't reach terminal state, bootstrap value target\n",
    "            if timeout or epoch_ended:\n",
    "                _, v, _ = ac.step(torch.as_tensor(o, dtype=torch.float32))\n",
    "            else:\n",
    "                v = 0\n",
    "            buf.finish_path(v)\n",
    "            frame, o, ep_ret, ep_len = get_screen(), env.reset(), 0, 0\n",
    "\n",
    "\n",
    "    # Save model\n",
    "    if (epoch % save_freq == 0) or (epoch == epochs-1):\n",
    "        torch.save(ac, f'./RL4_spinning_vpg')\n",
    "\n",
    "    # Perform VPG update!\n",
    "    update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d77854923f65db7c87e5b368fa3fd71bdf7a68487e1e3730c1972eb3f5d18515"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('RL')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
