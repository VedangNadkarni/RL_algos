{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym.spaces import Discrete, Box\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as optimizer\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib\n",
    "import torchvision.transforms as T\n",
    "from torch.distributions import Categorical, Normal\n",
    "from scipy import signal\n",
    "import scipy\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, h, w, outputs):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "\n",
    "        # Number of Linear input connections depends on output of conv2d layers\n",
    "        # and therefore the input image size, so compute it.\n",
    "        def conv2d_size_out(size, kernel_size = 5, stride = 2):\n",
    "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
    "        # print(convh, convw)\n",
    "        linear_input_size = convw * convh * 32\n",
    "        # self.head = nn.Linear(linear_input_size, outputs)\n",
    "        self.head = nn.Linear(linear_input_size, convw*convh)\n",
    "        # self.mid = nn.Linear(convw*convh/3, convh/2)\n",
    "        self.out = nn.Linear(convw*convh, outputs)\n",
    "        # self.out = F.softmax\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        # print(x)\n",
    "        # x = x.to(device)\n",
    "        # print('U', x[0])\n",
    "        # print('O', np.shape(x))\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        # return self.head(x.view(x.size(0), -1))\n",
    "        x = F.relu(self.head(x.view(x.size(0), -1)))\n",
    "        return F.softmax(self.out(x), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import InterpolationMode\n",
    "resize = T.Compose([T.ToPILImage(),\n",
    "                    T.Resize(40, interpolation=InterpolationMode.BICUBIC),\n",
    "                    T.ToTensor()])\n",
    "def get_cart_location(screen_width):\n",
    "    world_width = env.x_threshold * 2\n",
    "    scale = screen_width / world_width\n",
    "    return int(env.state[0] * scale + screen_width / 2.0)  # MIDDLE OF CART\n",
    "import time\n",
    "def get_screen():\n",
    "    # Returned screen requested by gym is 400x600x3, but is sometimes larger\n",
    "    # such as 800x1200x3. Transpose it into torch order (CHW).\n",
    "    # time.sleep(2)\n",
    "    screen = env.render(mode='rgb_array')\n",
    "    screen  = screen.transpose((2, 0, 1))\n",
    "    # screen = env.render(mode='rgb_array').transpose((2, 0, 1))\n",
    "    # Cart is in the lower half, so strip off the top and bottom of the screen\n",
    "    _, screen_height, screen_width = screen.shape\n",
    "    screen = screen[:, int(screen_height*0.4):int(screen_height * 0.8)]\n",
    "    view_width = int(screen_width * 0.6)\n",
    "    cart_location = get_cart_location(screen_width)\n",
    "    if cart_location < view_width // 2:\n",
    "        slice_range = slice(view_width)\n",
    "    elif cart_location > (screen_width - view_width // 2):\n",
    "        slice_range = slice(-view_width, None)\n",
    "    else:\n",
    "        slice_range = slice(cart_location - view_width // 2,\n",
    "                            cart_location + view_width // 2)\n",
    "    # Strip off the edges, so that we have a square image centered on a cart\n",
    "    screen = screen[:, :, slice_range]\n",
    "    # Convert to float, rescale, convert to torch tensor\n",
    "    # (this doesn't require a copy)\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    screen = torch.from_numpy(screen)\n",
    "    # Resize, and add a batch dimension (BCHW)\n",
    "    return resize(screen).unsqueeze(0).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "_, _, h, w = get_screen().shape\n",
    "# print(h, w, env.action_space)\n",
    "model = Net(h, w, env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(rewards, gamma=0.99):\n",
    "    # Cumulative discounted sum\n",
    "    r = np.array([gamma**i * rewards[i] \n",
    "                  for i in range(len(rewards))])\n",
    "    r = r[::-1].cumsum()[::-1]\n",
    "    # Subtracting the baseline reward \n",
    "    # Intuitively this means if the network predicts what it\n",
    "    # expects it should not do too much about it\n",
    "    # Stabalizes and speeds up the training \n",
    "    return r - r.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "tb = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01a3ea61d07b4dfcbda67269e49ae8a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 1500 Average of last 10: 28.10"
     ]
    }
   ],
   "source": [
    "total_rewards = []\n",
    "batch_rewards = []\n",
    "batch_actions = []\n",
    "batch_states = []\n",
    "batch_frames = []\n",
    "\n",
    "batch_counter = 1\n",
    "opt = optimizer.Adam(model.parameters(), 1e-2)\n",
    "action_space = np.arange(env.action_space.n)\n",
    "\n",
    "for ep in tqdm(range(1000, 1500)):\n",
    "    # Reset\n",
    "    s_0 = env.reset()\n",
    "    frame = get_screen()\n",
    "    states = []\n",
    "    rewards = []\n",
    "    actions = []\n",
    "    frames = []\n",
    "    complete = False\n",
    "    while complete == False:\n",
    "        # print(frame)\n",
    "        action_probs = model(torch.FloatTensor(frame)).detach().numpy().squeeze()\n",
    "        # action_probs = model(torch.FloatTensor(frame)).detach().numpy().squeeze()\n",
    "        # print(action_probs)\n",
    "        # action_probs = [0.5, 0.5]\n",
    "        action = np.random.choice(action_space, p=action_probs)\n",
    "        s1, r, complete, _ = env.step(action)\n",
    "        frame_ = get_screen()\n",
    "        states.append(s_0)\n",
    "        rewards.append(r)\n",
    "        actions.append(action)\n",
    "        frames.append(frame.squeeze())\n",
    "        s_0 = s1\n",
    "        frame = frame_\n",
    "        if complete:\n",
    "            batch_rewards.extend(discount_rewards(rewards, 0.99))\n",
    "            batch_states.extend(states)\n",
    "            batch_actions.extend(actions)\n",
    "            batch_frames.extend(frames)\n",
    "            batch_counter += 1\n",
    "            total_rewards.append(sum(rewards))\n",
    "            tb.add_scalar('Rewards', total_rewards[-1], ep)\n",
    "\n",
    "            if batch_counter == 10:\n",
    "                # Prepare the batches for training\n",
    "                # Add states, reward and actions to tensor\n",
    "                opt.zero_grad()\n",
    "                state_tensor = torch.FloatTensor(batch_states)\n",
    "                reward_tensor = torch.FloatTensor(batch_rewards)\n",
    "                action_tensor = torch.LongTensor(batch_actions)\n",
    "                # print(batch_states)\n",
    "                # print(batch_frames)\n",
    "                frame_tensor = torch.FloatTensor(batch_frames)\n",
    "\n",
    "                # Convert the probs by the model to log probabilities\n",
    "                log_probs = torch.log(model(frame_tensor))\n",
    "                # Mask the probs of the selected actions\n",
    "                selected_log_probs = reward_tensor * reward_tensor * log_probs[np.arange(len(action_tensor)), action_tensor]\n",
    "                # Loss is negative of expected policy function J = R * log_prob\n",
    "                loss = -selected_log_probs.mean()\n",
    "\n",
    "                # Do the update gradient descent(with negative reward hence is gradient ascent) \n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "                tb.add_scalar('2', loss.item(), ep)\n",
    "                batch_rewards = []\n",
    "                batch_actions = []\n",
    "                batch_states = []\n",
    "                batch_frames = []\n",
    "                batch_counter = 1\n",
    "\n",
    "            print(\"\\rEp: {} Average of last 10: {:.2f}\".format(\n",
    "                ep + 1, np.mean(total_rewards[-10:])), end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.zero_grad()\n",
    "opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, f'./RL_VPG_28k.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(5, 5), stride=(2, 2))\n",
       "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2): Conv2d(16, 32, kernel_size=(5, 5), stride=(2, 2))\n",
       "  (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3): Conv2d(32, 32, kernel_size=(5, 5), stride=(2, 2))\n",
       "  (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (head): Linear(in_features=512, out_features=16, bias=True)\n",
       "  (out): Linear(in_features=16, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "from PIL import Image\n",
    "\n",
    "def generate_gif(env, filename, T=200):\n",
    "    frames = []\n",
    "    s_t = env.reset()\n",
    "    reward = 0\n",
    "    for t in range(T):\n",
    "        # a_t = agent.act(s_t)\n",
    "        frame = get_screen()\n",
    "        action_probs = model(torch.FloatTensor(frame)).detach().numpy().squeeze()\n",
    "        # action_probs = model(torch.FloatTensor(frame)).detach().numpy().squeeze()\n",
    "        # print(action_probs)\n",
    "        # action_probs = [0.5, 0.5]\n",
    "        action = np.random.choice(action_space, p=action_probs)\n",
    "        s_t, r_t, d_t, _ = env.step(action)\n",
    "        frame_ = env.render(mode='rgb_array')\n",
    "        frames.append(frame_)\n",
    "        reward += r_t\n",
    "        if d_t:\n",
    "            break\n",
    "    print(reward)\n",
    "    images_list = [Image.fromarray(frame) for frame in frames]\n",
    "    imageio.mimsave(f'{filename}.gif', frames, duration=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.0\n"
     ]
    }
   ],
   "source": [
    "generate_gif(env, './Test2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d77854923f65db7c87e5b368fa3fd71bdf7a68487e1e3730c1972eb3f5d18515"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('RL')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
